---
title: "AllRunners"
output: html_document
---

---
title: "Testing full data set no avg"
output: html_document
date: "2025-12-11"
---

---
title: "EDA"
output: html_document
date: "2025-11-14"
---

Load all libraries (KB)
```{r}
library(here)
library(dplyr)
library(lubridate)
library(readr)
library(tibble)
library(knitr)
library(ggplot2)
library(tidyr)
library(corrplot)
library(VIM)
library(combinat)
library(xgboost)
library(Metrics)
```

Import all raw data (KB)
```{r}
weather = read.csv(here("data","raw-data","weather-data.csv"))
airquality = read.csv(here("data","raw-data","air-quality-data.csv"))
boston = read.csv(here("data","raw-data","boston-marathon-data.csv"))
chicago = read.csv(here("data","raw-data","chicago-marathon-data.csv"))
nyc = read.csv(here("data","raw-data","nyc-marathon-data.csv")) 
berlin = read.csv(here("data","raw-data","berlin-marathon-data.csv"))
```

Standardize all the raw marathon datasets; clean variable names and select necessary variables (KB)
```{r}
boston_clean <- boston %>%
  transmute(
    year = year,
    marathon = "Boston",
    gender = gender,
    chip_time = official_time
  )

berlin_clean <- berlin %>%
  transmute(
    year = YEAR,
    marathon = "Berlin",
    gender = GENDER,
    chip_time = TIME
  )

nyc_clean <- nyc %>%
  transmute(
    year = Year,
    marathon = "NYC",
    gender = Gender,
    chip_time = Finish.Time
  )

chicago_clean <- chicago %>%
  transmute(
    year = Year,
    marathon = "Chicago",
    gender = Gender,
    chip_time = Finish.Time
  )
```

Standardize the raw weather data; format the variables names to make them clean (KB)
```{r}
weather_clean <- weather %>%
  transmute(
    year = Year,
    marathon = Marathon,
    high_temp = High.Temp,
    low_temp = Low.Temp,
    avg_temp = Day.Average.Temp,
    precipitation = Precipitation,
    dew_point = Average.Dew.Point,
    wind_speed = Max.Wind.Speed,
    visibility = Visibility,
    sea_level_pressure = Sea.Level.Pressure
  )
```

Standardize the raw airquality data; format the variables names to make them clean (KB)
```{r}
airquality_clean <- airquality %>%
  transmute(
    year = Year,
    marathon = Marathon,
    aqi = Overall.AQI.Value,
    main_pollutant = Main.Pollutant,
    co = as.numeric(CO),
    ozone = as.numeric(Ozone),
    pm10 = suppressWarnings(as.numeric(PM10)),
    pm25 = as.numeric(PM2.5),
    no2 = as.numeric(NO2)
  )
```

Combine the marathon datasets (KB)
```{r}
# using bind_rows so we can row-wise combine and have data for each runner
marathons_all <- bind_rows(
  boston_clean,
  berlin_clean,
  nyc_clean,
  chicago_clean
)
#remove unused dfs to clean up environment
rm(boston_clean, boston, berlin_clean, berlin, nyc_clean, nyc, chicago_clean, chicago, weather, airquality)
```
We can see that marathons_all contains the identifying variables (*year, marathon, gender*), and the outcome variable *chip_time*.

Select only years we need (1996 to 2025) (KB)
```{r}
marathons_all <- marathons_all %>%
  filter(year >= 1996 & year <= 2025)

unique(marathons_all$year)
```
We can that the merged marathon data set now contains years from 1996 to 2023, as most dont have data up to 2025.

Chip-Time Cleaning Function (pulled from chatgbt): need chip_seconds to find the average finishing times (KB)
```{r}
# Cleans and standardizes chip-time values by converting Excel-style numeric
# times to HH:MM:SS, trimming and formatting text times, replacing invalid 
# entries with NA, and padding missing leading zeros.

clean_chip_time <- function(x) {

  # Convert numeric Excel-style times to character HH:MM:SS
  x_numeric <- suppressWarnings(as.numeric(x))
  is_fraction <- !is.na(x_numeric) & x_numeric < 1 & x_numeric > 0
  
  x[is_fraction] <- format(
    as.POSIXct("1970-01-01", tz = "UTC") + x_numeric[is_fraction] * 86400,
    "%H:%M:%S"
  )
  
  # Everything else treat as text and clean
  x <- as.character(x)
  x <- trimws(x)
  
  # Replace known invalid strings with NA
  invalid <- c("", "NA", "N/A", "—", "-", "DNF", "DNS", "DQ", "no time", "No Time", "NO TIME")
  x[x %in% invalid] <- NA
  
  # Pad missing zeros (H:MM:SS → HH:MM:SS, etc.)
  x <- gsub("^([0-9]):", "0\\1:", x)
  x <- gsub(":([0-9]):", ":0\\1:", x)
  x <- gsub(":([0-9])$", ":0\\1", x)
  
  return(x)
}
```

Clean chip_time; using `clean_chip_time` function (KB)
```{r}
marathons_all <- marathons_all %>%
  mutate(chip_time_clean = clean_chip_time(chip_time))
```

Convert the cleaned chip_time values (HH:MM:SS) to hms() and period_to_seconds() and gets a new column chip_seconds (KB)
```{r}
marathons_all <- marathons_all %>%
  mutate(
    chip_seconds = suppressWarnings(period_to_seconds(hms(chip_time_clean)))
  )

head(marathons_all)
```
We can see `chip_time` with the original data, `chip_time_clean` with the uniform cleaned data across all marathons, and `chip_seconds` with the total time in seconds. 

Need to see what we want to do with these missing values and where they are coming from (KB)
```{r}
marathons_all %>% 
  summarize(missing_finish_times = sum(is.na(chip_seconds)))

marathons_all %>% 
  filter(is.na(chip_seconds))
```
We can see that there are only 3 rows with missing chip_times.

Remove missing finish times. This is safe because we only use average finishing times in our model, so individual missing times do not matter. Also there are only 3 total. (KB)
```{r}
marathons_all <- marathons_all %>%
  filter(!is.na(chip_seconds))
```

Labeling Gender as 'male', 'female', or 'unknown', and we decided that nonbinary falls under female. (KB)
```{r}
# Check all unique names under gender
unique(marathons_all$gender)
```
We can see that there are a lot of ways 'male', 'female', 'nonbinary', and 'unknown' are labled.

Make all the genders uniform and standardized (KB)
```{r}
marathons_all <- marathons_all %>%
  mutate(
    gender = tolower(gender),
    gender = case_when(
      gender %in% c("male", "m") ~ "male",
      gender %in% c("female", "f", "w", "x", "nonbinary", "nb") ~ "female",
      TRUE ~ "unknown"
    )
  )

table(marathons_all$gender)
```
We can see that now we only have three genders, `female`, `male`, and `unknown` which consist of 56 rows.  

Remove unknowns since we have only 56 unknowns, and they will not help the model and most likely add noise: (KB)
```{r}
marathons_all <- marathons_all %>% 
  filter(gender != "unknown")

table(marathons_all$gender)
```
We successfully removed the unknowns, leaving us with the desired `female` and `male`. 

Create a variable called winner_time, that consist of the winners or best finishing for that year. Then create a variable called time_ratio (), that consists of chip_seconds / winner_time. (KB)
```{r}
# Add winner_time for each marathon-year-gender
runners <- marathons_all %>%
  group_by(marathon, year, gender) %>%
  mutate(winner_time = min(chip_seconds, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(time_ratio = chip_seconds / winner_time)
```

Create subgroups based on time_ratio. Look at a histogram to adjust the boundaries of the subgroups based on the distribution of the histogram of time_ratio and clear clusters. (KB)
```{r}
runners <- runners %>%
  mutate(
    subgroup = case_when(
      time_ratio <= 1.30 ~ "elite",
      time_ratio > 1.30 & time_ratio <= 1.55 ~ "competitive",
      time_ratio > 1.55 & time_ratio <= 1.80 ~ "average",
      time_ratio > 1.80 & time_ratio <= 2.10 ~ "recreational",
      time_ratio > 2.10 ~ "slow",
      TRUE ~ NA_character_
    ),
    subgroup = factor(subgroup, levels = c("elite", "competitive", "average", "recreational", "slow"))
  )

# Create histogram of ratio_time amongst male and female runners with boundaries for subgroups in red
ggplot(runners, aes(x = time_ratio, fill = gender)) +
  geom_histogram(binwidth = 0.05, position = "dodge") +
  scale_fill_manual(values = c("female" = "deeppink", "male" = "steelblue")) +
  geom_vline(xintercept = c(1.3, 1.55, 1.80, 2.10), linetype = "dashed", color = "red") +
  labs(
    title = "Distribution of Runner Time Ratios to Winner by Gender",
    x = "Time Ratio (Runner / Winner)",
    y = "Count"
  ) +
  theme_minimal()
```
Looking at the histogram of the time_ratio above, we can see that there is a clear clustering in runner-to-winner time ratios and a long right-skewed tail, which helped with the placement of the performance thresholds/ vertical dashed red lines (1.30, 1.55, 1.80, 2.10). We can also see that the distribution for both male and female follow a similar shape, with overall less female relative male runners. Overall, this figure supports the use of ratio-based performance categories by illustrating natural clustering and skewness in marathon finishing times.

Now we can further divide the subgroups created by genders. (KB)
```{r}
runners %>%
  group_by(gender, subgroup) %>%
  summarise(count = n()) %>%
  arrange(gender, subgroup)
```
We can see that the subgroups now have groups for `male` and `female`. We can see that the elite groups both has less runners and male runners have more runners in each group overall. We will leave this as is because these cutoffs are not arbitrary, and they follow the natural shape of the data rather than relying on fixed percentile breaks. Weighting could later be applied; however, it is not initially because subgroup effects are not the primary focus.


#Compute average finishing time per subgroup (KB)
##COMMENTING THIS OUT TO MAKE DATASET BIGGER
#```{r}
#avg_times <- runners %>%
#  group_by(marathon, year, gender, subgroup) %>%
#  summarize(
#    n = n(),  # number of runners in each subgroup
#    avg_chip_seconds = mean(chip_seconds, na.rm = TRUE),  
#    .groups = "drop"
#  )

#avg_times
#```

#removing irrelevant columns that were used to calculate subgroup
```{r}
runners <- subset(runners, select = -c(chip_time, chip_time_clean, winner_time, time_ratio))
```




We now have a dataset that contains 1005 rows and 6 columns/ variables. This means that some of the years must be missing for some of the marathon as we know previously when collecting the raw data. Berlin goes from (1996 to 2019), Boston goes from (1996 to 2019), Chicago goes from (1996 to 2023), and NYC goes from (1996 to 2024). Also, after looking through the dataset more, we can see that Berlin is missing data from all female subgroups in 2019 and NYC is missing all male subgroups for 2024. We will leave the data as is because having missing years and genders is not our primary focus, as our focus is on having as much data for finishing/chip times as possible.

Left join weather_clean and airquailty_clean onto the cleaned marathon datasets (KB)
```{r}
final_data <- runners %>%
  left_join(weather_clean, by = c("year", "marathon")) %>%
  left_join(airquality_clean, by = c("year", "marathon"))

#COMMENTING OUT SINCE IT'S NO LONGER RELEVANT
# move n (number of individuals representing each group) to the first column for neatness
#final_data <- final_data %>% 
#  select(n, everything())

rm(airquality_clean, runners, marathons_all, runners, clean_chip_time,weather_clean)

```
We can see that all the datasets were merged into one final_data successfully. We now have a new column `n` that counts the amount of runners in each group for future computations and potential weighing if needed. 





#Now we are pivoting to feat. engineering

```{r}
# Impute missing PM2.5 values using 5 nearest neighbors
#Approximate Nearest Neighbor
library(RANN)

#predictors for pm2.5
predictors <- c("avg_temp", "dew_point", "wind_speed")

# only complete-case rows for training
train <- final_data[!is.na(final_data$pm25), ]
test  <- final_data[is.na(final_data$pm25), ]

# Compute nearest neighbors
nn <- nn2(train[, predictors], test[, predictors], k = 5)

# Extract PM2.5 of neighbors and ensure matrix shape
neighbors <- matrix(train$pm25[nn$nn.idx], ncol = 5)

# impute as mean of neighbors
final_data$pm25[is.na(final_data$pm25)] <- rowMeans(neighbors)

# Check that missing values are filled
summary(final_data$pm25)
```
Convert categorical variables to factors: (KB)
```{r}
final_data  <- final_data  %>%
  mutate(subgroup = factor(subgroup),
         gender = factor(gender),
         marathon = factor(marathon),
         main_pollutant = factor(main_pollutant))

```

We can see that the identifiers (subgroup, gender, marathon) and predictor (main_pollutant) were all converted to factors so that our models can properly recognized them. 

# Feature engineering (ZD)
```{r}
# create interaction terms and convert supershoe to factor
final_data <- final_data %>%
  mutate(
    supershoe = factor(ifelse(year >= 2018, 1, 0), levels = c(0, 1)),
    temp_dew_interaction       = avg_temp * dew_point,
    temp_aqi_interaction       = avg_temp * aqi, 
    temp_precip_interaction    = avg_temp * precipitation,
    temp_wind_interaction      = avg_temp * wind_speed,
    pm25_temp_interaction      = pm25 * avg_temp,
    dew_wind_interaction       = dew_point * wind_speed,
    pressure_temp_interaction  = sea_level_pressure * avg_temp,
    avg_temp_gender_interaction = avg_temp * as.numeric(gender == "male")
  )

```

# Need to do a Quick correlation check for numeric variables and see what features inroduce multicollinearity (KB)
```{r}
# Find numeric columns after feature engineering, excluding 'year' and 'n' since they are idenifiers
numeric_vars <- final_data %>%
  select(where(is.numeric)) %>%
  select(-year) %>% 
  names()

numeric_vars
```
We can see that there are a total 14 continuous variables, disincluding the identifiers `year` and `n`. 

Creat Correlation Matrix and make a visual: (KB)
```{r}
cor_matrix <- cor(final_data[numeric_vars], use = "pairwise.complete.obs")

# rounding
round(cor_matrix, 2)

#ploting
corrplot(
  cor_matrix,
  method = "color",
  col = colorRampPalette(c("blue", "white", "red"))(200),
  tl.cex = 0.6
)
```

Removing correlated variables as seen above: (KB)
```{r}
cols_to_remove <- c(
  "high_temp",
  "low_temp",
  "aqi",
  "temp_dew_interaction",
  "temp_precip_interaction",
  "temp_wind_interaction",
  "pm25_temp_interaction",
  "dew_wind_interaction",
  "pressure_temp_interaction",
  "main_pollutant" 
)

final_data <- final_data[, !(names(final_data) %in% cols_to_remove)]


numeric_vars <- final_data %>%
  select(where(is.numeric)) %>%
  select(-year) %>% 
  names()

cor_matrix <- cor(final_data[numeric_vars], use = "pairwise.complete.obs")

# rounding
round(cor_matrix, 2)

#ploting
corrplot(
  cor_matrix,
  method = "color",
  col = colorRampPalette(c("blue", "white", "red"))(200),
  tl.cex = 0.6
)

```



Since PM10 has a lot of missing values still, we will drop that column completely: (KB)
```{r}
final_data <- final_data %>%
  select(-pm10) 
```


Make sure marathon, gender,subgroup, and supershoe are all factors (KB)
```{r}
final_data$supershoe <- factor(final_data$supershoe)
final_data$marathon <- factor(final_data$marathon)
final_data$gender   <- factor(final_data$gender)
final_data$subgroup <- factor(final_data$subgroup)
```

```{r}
#Separate Berlin and main data 
main_data <- final_data %>% filter(marathon != "Berlin")
berlin_data <- final_data %>% filter(marathon == "Berlin")
```


```{r}
# split data into train and test
set.seed(123)

train_index <- sample(1:nrow(main_data), size = 0.9 * nrow(main_data)) # use a 90/10 split

train_data <- main_data[train_index, ]
test_data  <- main_data[-train_index, ]
```


# Data Scaling (for linear regression model): (ZD, KB)

```{r}
# Identify numeric predictors to scale (exclude outcome and identifiers)
numeric_vars <- train_data %>%
  select(where(is.numeric)) %>%
  select(-chip_seconds, -year) %>%
  names()

# scale training data and save scaling parameters
train_scaled <- scale(train_data[numeric_vars])
train_data[paste0("scaled_", numeric_vars)] <- train_scaled

train_center <- attr(train_scaled, "scaled:center")
train_scale  <- attr(train_scaled, "scaled:scale")

# scale test data using training parameters
test_scaled <- scale(test_data[numeric_vars],
                     center = train_center,
                     scale  = train_scale)
test_data[paste0("scaled_", numeric_vars)] <- test_scaled


summary(train_data[paste0("scaled_", numeric_vars)])

```

# Adding Binned features based off Decision Tree (ozone_bin and pm25_bin) (KB)
```{r}
# Training data 
train_data$ozone_bin <- factor(ifelse(train_data$ozone >= 38, 1, 0), levels = c(0, 1))
train_data$pm25_bin  <- factor(ifelse(train_data$pm25 >= 54, 1, 0), levels = c(0, 1))

# Test data using same thresholds
test_data$ozone_bin <- factor(ifelse(test_data$ozone >= 38, 1, 0), levels = c(0, 1))
test_data$pm25_bin  <- factor(ifelse(test_data$pm25 >= 54, 1, 0), levels = c(0, 1))
```

We made the factor levels:  
- ozone bin: 0 = low, 1 = high
- pm25 bin: 0 = low, 1 = high


From Lasso on dataset when all runners were grouped with avg_chip_time, both binned features and scaled_pm25 were retained showing (.). We can also see that scaled_ozone shrank to 0, which means it did not contribute to the predicting average chip time once the binned values were also included. The model achieved an RMSE of 3383 seconds and a MAE of 2861 seconds, suggesting that good predictive performance. 

Removing ozone based on LASSO results (KB)
```{r}
# Remove scaled_ozone from training and test data
train_data <- train_data[, !(names(train_data) %in% c("scaled_ozone","ozone"))]
test_data  <- test_data[, !(names(test_data) %in% c("scaled_ozone","ozone"))]
```

We can see that scaled ozone was successfully removed from both train and test data. 


```{r}
#Initial Model (MH)
lm_interact <- lm(
  log(chip_seconds) ~ marathon + supershoe + gender + subgroup +
    (scaled_avg_temp + scaled_precipitation + scaled_dew_point +
     scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure +
     scaled_co + scaled_pm25 + ozone_bin + scaled_no2) * gender +
    (scaled_avg_temp + scaled_precipitation + scaled_dew_point +
     scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure +
     scaled_co + scaled_pm25 + ozone_bin + scaled_no2) * subgroup,
  data = train_data
)


lm_initial_scaledPM25 <- lm(log(chip_seconds) ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure + scaled_co + scaled_pm25 + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = train_data)

anova(lm_initial_scaledPM25, lm_interact)

summary(lm_interact)
plot(lm_interact)

#lm_initial_PM25bin <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + #scaled_visibility + scaled_sea_level_pressure + scaled_co + pm25_bin + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = #train_data)


summary(lm_initial_scaledPM25)
plot(lm_initial_scaledPM25)

```

```{r}
#Adding RMSE and MAE for looking at overfitting with test results below (MH/ZD)
residuals_lm_initial_PM25bin <- lm_interact$residuals
residuals_lm_initial_PM25bin_rmse <- sqrt(mean(residuals_lm_initial_PM25bin^2))
residuals_lm_initial_PM25bin_mae <- mean(abs(residuals_lm_initial_PM25bin))

residuals_lm_initial_scaledPM25 <- lm_initial_scaledPM25$residuals
residuals_lm_initial_scaledPM25_rmse <- sqrt(mean(residuals_lm_initial_scaledPM25^2))
residuals_lm_initial_scaledPM25_mae <- mean(abs(residuals_lm_initial_scaledPM25))



model_compare_train <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE  = c(residuals_lm_initial_scaledPM25_rmse,
            residuals_lm_initial_PM25bin_rmse),
  MAE   = c(residuals_lm_initial_scaledPM25_mae,
            residuals_lm_initial_PM25bin_mae),
  R2    = c(summary(lm_initial_scaledPM25)$r.squared,
            summary(lm_interact)$r.squared)
)

# Display nicely formatted table
knitr::kable(model_compare_train, digits = 4,
             caption = "Training Data Comparison: Continuous vs Binned PM2.5")

```

```{r}
# More Modeling (ZD)

# build a function
evaluate <- function(model, test_data) {
  preds <- predict(model, newdata = test_data)
  actual <- test_data$chip_seconds
  
  rmse <- sqrt(mean((preds - actual)^2))
  mae  <- mean(abs(preds - actual))
  r2   <- cor(preds, actual)^2
  
  return(list(RMSE = rmse, MAE = mae, R2 = r2))
}

# Evaluate Model 1 with scaled PM2.5
results_scaledPM25 <- evaluate(lm_initial_scaledPM25, test_data)

# Evaluate Model 2 with PM2.5 bin
results_lm_interact <- evaluate(lm_interact, test_data)
```

```{r}
# compare results (ZD)
model_compare <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE = c(results_scaledPM25$RMSE, results_lm_interact$RMSE),
  MAE  = c(results_scaledPM25$MAE,  results_lm_interact$MAE),
  R2   = c(results_scaledPM25$R2,   results_lm_interact$R2)
)

knitr::kable(model_compare, digits = 4,
             caption = "Comparison of Initial Models: Continuous vs Binned PM2.5")
```



#Get main data but without scaling for XGBoost

```{r}
# split data into train and test
set.seed(123)

train_index <- sample(1:nrow(main_data), size = 0.9 * nrow(main_data)) # use a 90/10 split

train_data2 <- main_data[train_index, ]
test_data2  <- main_data[-train_index, ]
```


```{r}
# Remove identifiers before creating model matrix
# Convert dataset to numeric-only matrix
train_matrix <- model.matrix(
  chip_seconds ~ .,
  data = train_data2 %>% select(-year)
)

test_matrix <- model.matrix(
  chip_seconds ~ .,
  data = test_data2 %>% select(-year)
)

# Convert to DMatrix format for tuning, needed for cross validation
dtrain <- xgb.DMatrix(data = train_matrix, label = train_data2$chip_seconds)
dtest  <- xgb.DMatrix(data = test_matrix,  label = test_data2$chip_seconds)
```


```{r}
# Create an untuned XGBoost model
xgb_base <- xgboost(
  data = dtrain,
  nrounds = 200,
  objective = "reg:squarederror",
  verbose = FALSE # mute iterations
)

pred_xgb <- predict(xgb_base, dtest)

rmse_xgb <- rmse(test_data2$chip_seconds, pred_xgb)
mae_xgb  <- mae(test_data2$chip_seconds, pred_xgb)

c(RMSE = rmse_xgb, MAE = mae_xgb)
```

This represents the initial overfitting model prior to tuning (ZD, KB)
```{r}
# Zack's original attempt

xgb_cv_of <- xgb.cv(
  data = dtrain,
  nrounds = 2000,
  nfold = 5,
  objective = "reg:squarederror",
  eta = 0.01,
  max_depth = 4,
  subsample = 0.8,
  colsample_bytree = 0.8,
  early_stopping_rounds = 50,
  verbose = 0 # mute iterations
)

xgb_cv_of$best_iteration

# Train model
params_of <- list(
  objective = "reg:squarederror",
    eta = 0.01,
    max_depth = 4,
    subsample = 0.8,
    colsample_bytree = 0.8
)
# Find best # of trees from cv
best_xgb_of <- xgb.train(
  params = params_of,
  data = dtrain,
  nrounds = xgb_cv_of$best_iteration #2000
)
```
We can see that the XGBoost model performed best at 2000 trees according to cross-validation, with is hitting the max nrounds. 

Checking to see if the model is overfitting (ZD, KB)
```{r}
# Overfitting?
cv_rmse_of <- min(xgb_cv_of$evaluation_log$test_rmse_mean)
train_rmse_of <- min(xgb_cv_of$evaluation_log$train_rmse_mean)

data.frame(
  Train_RMSE = train_rmse_of,
  CV_RMSE = cv_rmse_of,
  Test_RMSE = rmse_xgb
)
```

```









