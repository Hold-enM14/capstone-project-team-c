---
title: "Preprocessing and Feature Engineering"
date: "2025-11-20"
output: html_document
---

Load libraries (KB, ZD, MH)
```{r}
library(here)
library(dplyr)
library(knitr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(VIM)
library(readr)
```

Import data (KB)
```{r}
final_data <- read.csv(here("data", "merged_marathon_data.csv"))
```

(ZD)
```{r}
# view merged data
str(final_data)
summary(final_data)
```

Look at the full dataset for missing values (ZD)
```{r}
# missing values for visibility, co, pm10, pm2.5
missing_summary <- final_data %>%
  summarise(across(everything(), ~sum(is.na(.))))

knitr::kable(missing_summary, caption = "Missing Values per Variable")
```

 Split the data so Berlin is used as a second case study to show how the method performs with missing data (whether successful or not). (KB)
```{r}
main_data <- final_data %>% filter(marathon != "Berlin")
berlin_data <- final_data %>% filter(marathon == "Berlin")

str(main_data)
```
We can see the main_data with out berlin now has 770 obs. and 21 variables. 

#Handling Missing Values (ZD, KB)
```{r}
# missing values
missing_summary <- main_data %>%
  summarise(across(everything(), ~sum(is.na(.))))

knitr::kable(missing_summary, caption = "Missing Values per Variable")
```

In the main_data, we can see that there are still 320 missing values for pm10 and 70 missing values for pm2.5. 

Since PM10 has a lot of missing values still, we will drop that column completely: (KB)
```{r}
main_data <- main_data %>%
  select(-pm10) 
```

Since PM2.5 is often the main pollutant, we decided to use KNN imputation to fill missing PM2.5 values because it predicts missing data using similar rows without assuming a specific parametric relationship, preserves variance, and works well for our relatively small dataset while using correlations with other environmental variables: (KB)
```{r}
# Impute missing PM2.5 values using 5 nearest neighbors
main_data <- kNN(main_data, variable = "pm25", k = 5) # can change later to see which K gives best model performance 

# remove pm25_imp
cols_to_remove <- c(
  "pm25_imp"
)

main_data <- main_data[, !(names(main_data) %in% cols_to_remove)]

# Check that missing values are filled
summary(main_data$pm25)
```
We can see that there are no missing values now and we can get a clean summary of the data. 

Convert categorical variables to factors: (KB)
```{r}
main_data  <- main_data  %>%
  mutate(subgroup = factor(subgroup),
         gender = factor(gender),
         marathon = factor(marathon),
         main_pollutant = factor(main_pollutant))

str(main_data )
```
We can see that the identifiers (subgroup, gender, marathon) and predictor (main_pollutant) were all converted to factors so that our models can properly recognized them. 

# Feature engineering (ZD)
```{r}
# create interaction terms and convert supershoe to factor
main_data <- main_data %>%
  mutate(
    supershoe = factor(ifelse(year >= 2018, 1, 0), levels = c(0, 1)),
    temp_dew_interaction       = avg_temp * dew_point,
    temp_aqi_interaction       = avg_temp * aqi, 
    temp_precip_interaction    = avg_temp * precipitation,
    temp_wind_interaction      = avg_temp * wind_speed,
    pm25_temp_interaction      = pm25 * avg_temp,
    dew_wind_interaction       = dew_point * wind_speed,
    pressure_temp_interaction  = sea_level_pressure * avg_temp,
    avg_temp_gender_interaction = avg_temp * as.numeric(gender == "male")
  )

str(main_data)
```

# Need to do a Quick correlation check for numeric variables and see what features inroduce multicollinearity (KB)
```{r}
# Find numeric columns after feature engineering, excluding 'year' and 'n' since they are idenifiers
numeric_vars <- main_data %>%
  select(where(is.numeric)) %>%
  select(-year, -n) %>% 
  names()

numeric_vars
```
We can see that there are a total 14 continuous variables, disincluding the identifiers `year` and `n`. 

Creat Correlation Matrix and make a visual: (KB)
```{r}
cor_matrix <- cor(main_data[numeric_vars], use = "pairwise.complete.obs")

# rounding
round(cor_matrix, 2)

#ploting
corrplot(
  cor_matrix,
  method = "color",
  col = colorRampPalette(c("blue", "white", "red"))(200),
  tl.cex = 0.6
)
```
Looking at the visual, we can see that a lot of these features were highly correlated and will have to be removed. For example, high_temp and low_temp are strongly correlated with avg_temp (0.93 and 0.92), and aqi overlaps with pm25 (0.94). Interaction terms like temp_dew_interaction, temp_precip_interaction, and pressure_temp_interaction show very high correlations with their constituent variables (up to 1.00), making them redundant. Similarly, main_pollutant is just a categorical version of aqi. Overall, these features provide little additional information and can be removed to reduce multicollinearity. 

Observing categorical variables and their distributions to see if we should keep or remove: (KB)
```{r}
table(main_data$main_pollutant)
table(main_data$supershoe)
```
We can see that main_pollutant still contains data from pm10 which was removed. Also we know that main_pollutant is the categorical variable for aqi, and since aqi is highly correlated with pm2.5 (often the main_pollutant), we will drop aqi and main_pollutant all together. 

Looking at the supershoes variable, there is an expected imbalance, with more observations of years without supershoes (0 = 640) and fewer with supershoes (1 = 130) since they were introduced more recently in 2018. We will keep this as a control variable, and if needed, techniques like weighting can be applied to help the imbalance.

Removing correlated variables as seen above: (KB)
```{r}
cols_to_remove <- c(
  "high_temp",
  "low_temp",
  "aqi",
  "temp_dew_interaction",
  "temp_precip_interaction",
  "temp_wind_interaction",
  "pm25_temp_interaction",
  "dew_wind_interaction",
  "pressure_temp_interaction",
  "main_pollutant" 
)

main_data <- main_data[, !(names(main_data) %in% cols_to_remove)]
str(main_data)

```
Now we have a dataset that has 770 obs. and only 19 variables after the addition of newly engineered features and removal if highly correlated features. 


Export final prepossessing data to csv; undo and add the `#` before and after saving the main_data file (KB)
```{r}
#write_csv(main_data, "pre_feat_data.csv")
```



------------------------
Dictionary:

  ``` Contains code ideas that were not used.
  ```
  
# indicate and define year of covid (ZD)
final_data <- final_data %>%
  mutate(
    covid_era = ifelse(year == 2020, 1, 0)
  )


# replacing the numeric variables with missing data with mean (could do median if we keep this); most likely dropping though (ZD)
numeric_vars <- final_data %>%
  select(where(is.numeric)) %>%
  names()

final_data <- final_data %>%
  mutate(across(all_of(numeric_vars), ~ifelse(is.na(.), mean(., na.rm = TRUE), .)))



# create custom cutoff times for performance groups (ZD)
final_data <- final_data %>%
  mutate(
    finish_hours = avg_chip_seconds / 3600, # think we are having all time in seconds, need fixing
    subgroup2 = case_when(
      finish_hours < 3 ~ "elite",
      finish_hours < 3.75 ~ "competitive",
      finish_hours < 4.75 ~ "average",
      finish_hours < 5.75 ~ "recreational",
      TRUE ~ "slow"
    )
  )
