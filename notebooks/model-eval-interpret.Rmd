---
title: "Model Evaluation and Interpretation"
author: "Team C"
date: "2025-12-04"
output: html_document
---

# Load libraries
```{r}
library(here)
library(dplyr)
library(knitr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(VIM)
library(combinat)
library(xgboost)
library(Metrics)
```

# Importing the data (KB)
```{r}
main_data <- read.csv(here("data", "pre_feat_data.csv"))
```

Make sure marathon, gender,subgroup, and supershoe are all factors (KB)
```{r}
main_data$supershoe <- factor(main_data$supershoe)
main_data$marathon <- factor(main_data$marathon)
main_data$gender   <- factor(main_data$gender)
main_data$subgroup <- factor(main_data$subgroup)

str(main_data)
```
Now all characters are factors ready for modeling. 

# Add our 90/10 training/test split (ZD)
```{r}
# split data into train and test
set.seed(123)

train_index <- sample(1:nrow(main_data), size = 0.9 * nrow(main_data)) # use a 90/10 split

train_data <- main_data[train_index, ]
test_data  <- main_data[-train_index, ]
```


# Data Scaling (for linear regression model): (ZD, KB)

```{r}
# Identify numeric predictors to scale (exclude outcome and identifiers)
numeric_vars <- train_data %>%
  select(where(is.numeric)) %>%
  select(-avg_chip_seconds, -year, -n) %>%
  names()

# scale training data and save scaling parameters
train_scaled <- scale(train_data[numeric_vars])
train_data[paste0("scaled_", numeric_vars)] <- train_scaled

train_center <- attr(train_scaled, "scaled:center")
train_scale  <- attr(train_scaled, "scaled:scale")

# scale test data using training parameters
test_scaled <- scale(test_data[numeric_vars],
                     center = train_center,
                     scale  = train_scale)
test_data[paste0("scaled_", numeric_vars)] <- test_scaled


summary(train_data[paste0("scaled_", numeric_vars)])

```


```{r}
summary(test_data[paste0("scaled_", numeric_vars)])
```


We can see that all the continuous variables for the train and test data, not including the identifying variables `year` and `n`, were successfully scaled with means at 0 and standard deviations of 1, showing that the standardization was correctly applied.

# Adding Binned features based off Decision Tree (ozone_bin and pm25_bin) (KB)
```{r}
# Training data 
train_data$ozone_bin <- factor(ifelse(train_data$ozone >= 38, 1, 0), levels = c(0, 1))
train_data$pm25_bin  <- factor(ifelse(train_data$pm25 >= 54, 1, 0), levels = c(0, 1))

# Test data using same thresholds
test_data$ozone_bin <- factor(ifelse(test_data$ozone >= 38, 1, 0), levels = c(0, 1))
test_data$pm25_bin  <- factor(ifelse(test_data$pm25 >= 54, 1, 0), levels = c(0, 1))
```

We made the factor levels:  
- ozone bin: 0 = low, 1 = high
- pm25 bin: 0 = low, 1 = high


```{r}
str(train_data)
```



# LASSO is being used to answer the following question: (KB)

Which version of the air-quality and temperature features should we keep for the final model?
```{r}
library(glmnet)

# Prepare the predictor matrix (x) and outcome vector (y)
# Only using ozone and PM2.5 (continuous and binned)
x_train <- model.matrix(avg_chip_seconds ~ scaled_ozone + scaled_pm25 + 
                        ozone_bin + pm25_bin, data = train_data)[,-1]  # remove intercept
y_train <- train_data$avg_chip_seconds

x_test <- model.matrix(avg_chip_seconds ~ scaled_ozone + scaled_pm25 + 
                       ozone_bin + pm25_bin, data = test_data)[,-1]
y_test <- test_data$avg_chip_seconds

# Run LASSO with cross-validation to select lambda
set.seed(123)
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, standardize = FALSE)  # already scaled

# Find the best lambda
best_lambda <- lasso_cv$lambda.min
best_lambda

# Fit final LASSO model at best lambda
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda, standardize = FALSE)

# Check coefficients
coef(lasso_model)

# Predict on test set
preds <- predict(lasso_model, newx = x_test)

# Evaluate performance
rmse <- sqrt(mean((y_test - preds)^2))
mae  <- mean(abs(y_test - preds))
rmse
mae

```


We can see that both binned features and scaled_pm25 were retained showing (.). We can also see that scaled_ozone shrank to 0, which means it did not contribute to the predicting average chip time once the binned values were also included. The model achieved an RMSE of 3383 seconds and a MAE of 2861 seconds, suggesting that good predictive performance. 

Removing ozone based on LASSO results (KB)
```{r}
# Remove scaled_ozone from training and test data
train_data <- train_data[, !(names(train_data) %in% c("scaled_ozone"))]
test_data  <- test_data[, !(names(test_data) %in% c("scaled_ozone"))]

str(train_data)
str(test_data)
```

We can see that scaled ozone was successfully removed from both train and test data. 






(I ENDED HERE... I made the pre-feat-data into a csv and added the scaling and adding bins above)


```{r}
#Initial Model (MH)
lm_initial_scaledPM25 <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure + scaled_co + scaled_pm25 + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = train_data)


summary(lm_initial_scaledPM25)
plot(lm_initial_scaledPM25)

lm_initial_PM25bin <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure + scaled_co + pm25_bin + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = train_data)


summary(lm_initial_PM25bin)
plot(lm_initial_PM25bin)

```

```{r}
#Adding RMSE and MAE for looking at overfitting with test results below (MH/ZD)
residuals_lm_initial_PM25bin <- lm_initial_PM25bin$residuals
residuals_lm_initial_PM25bin_rmse <- sqrt(mean(residuals_lm_initial_PM25bin^2))
residuals_lm_initial_PM25bin_mae <- mean(abs(residuals_lm_initial_PM25bin))

residuals_lm_initial_scaledPM25 <- lm_initial_scaledPM25$residuals
residuals_lm_initial_scaledPM25_rmse <- sqrt(mean(residuals_lm_initial_scaledPM25^2))
residuals_lm_initial_scaledPM25_mae <- mean(abs(residuals_lm_initial_scaledPM25))



model_compare_train <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE  = c(residuals_lm_initial_scaledPM25_rmse,
            residuals_lm_initial_PM25bin_rmse),
  MAE   = c(residuals_lm_initial_scaledPM25_mae,
            residuals_lm_initial_PM25bin_mae),
  R2    = c(summary(lm_initial_scaledPM25)$r.squared,
            summary(lm_initial_PM25bin)$r.squared)
)

# Display nicely formatted table
knitr::kable(model_compare_train, digits = 4,
             caption = "Training Data Comparison: Continuous vs Binned PM2.5")

```





```{r}
# More Modeling (ZD)

# build a function
evaluate <- function(model, test_data) {
  preds <- predict(model, newdata = test_data)
  actual <- test_data$avg_chip_seconds
  
  rmse <- sqrt(mean((preds - actual)^2))
  mae  <- mean(abs(preds - actual))
  r2   <- cor(preds, actual)^2
  
  return(list(RMSE = rmse, MAE = mae, R2 = r2))
}

# Evaluate Model 1 with scaled PM2.5
results_scaledPM25 <- evaluate(lm_initial_scaledPM25, test_data)

# Evaluate Model 2 with PM2.5 bin
results_PM25bin <- evaluate(lm_initial_PM25bin, test_data)
```

```{r}
# compare results (ZD)
model_compare <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE = c(results_scaledPM25$RMSE, results_PM25bin$RMSE),
  MAE  = c(results_scaledPM25$MAE,  results_PM25bin$MAE),
  R2   = c(results_scaledPM25$R2,   results_PM25bin$R2)
)

knitr::kable(model_compare, digits = 4,
             caption = "Comparison of Initial Models: Continuous vs Binned PM2.5")
```

```{r}
# build residual plots (ZD)
plot(lm_initial_PM25bin, which = 1)   # Residuals vs Fitted
plot(lm_initial_PM25bin, which = 2)   # Q–Q plot
```









Start of Model Evaluation and Interpretation section:
(Attempt)

```{r}
# Remove identifiers before creating model matrix
# Convert dataset to numeric-only matrix
train_matrix <- model.matrix(
  avg_chip_seconds ~ .,
  data = train_data %>% select(-year, -n)
)

test_matrix <- model.matrix(
  avg_chip_seconds ~ .,
  data = test_data %>% select(-year, -n)
)

# Convert to DMatrix format for tuning, needed for cross validation
dtrain <- xgb.DMatrix(data = train_matrix, label = train_data$avg_chip_seconds)
dtest  <- xgb.DMatrix(data = test_matrix,  label = test_data$avg_chip_seconds)
```

```{r}
# Create an untuned XGBoost model
xgb_base <- xgboost(
  data = dtrain,
  nrounds = 200,
  objective = "reg:squarederror",
  verbose = FALSE # mute iterations
)

pred_xgb <- predict(xgb_base, dtest)

rmse_xgb <- rmse(test_data$avg_chip_seconds, pred_xgb)
mae_xgb  <- mae(test_data$avg_chip_seconds, pred_xgb)

c(RMSE = rmse_xgb, MAE = mae_xgb)
```

```{r}
# Introduce cross validation hyperparameter tuning
xgb_cv <- xgb.cv(
  data = dtrain,
  nrounds = 2000,
  nfold = 5,
  objective = "reg:squarederror",
  eta = 0.01,
  max_depth = 4,
  subsample = 0.8,
  colsample_bytree = 0.8,
  early_stopping_rounds = 50,
  verbose = 0 # mute iterations
)

xgb_cv$best_iteration
```

```{r}
# Train tuned model
params <- list(
  objective = "reg:squarederror",
  eta = 0.01,
  max_depth = 4,
  subsample = 0.8,
  colsample_bytree = 0.8
)
# Find best # of trees from cv
best_xgb <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = xgb_cv$best_iteration
)
```


```{r}
# Evaluate tuned model
pred_best <- predict(best_xgb, dtest)

rmse_best <- rmse(test_data$avg_chip_seconds, pred_best)
mae_best  <- mae(test_data$avg_chip_seconds, pred_best)

c(RMSE = rmse_best, MAE = mae_best)
```

```{r}
# Variable importance amongst XGBoosted model
importance <- xgb.importance(model = best_xgb)
print(importance)

xgb.plot.importance(importance)
```

```{r}
# Overfitting?
cv_rmse <- min(xgb_cv$evaluation_log$test_rmse_mean)
train_rmse <- min(xgb_cv$evaluation_log$train_rmse_mean)

data.frame(
  Train_RMSE = train_rmse,
  CV_RMSE = cv_rmse,
  Test_RMSE = rmse_best
)
```
```{r}
#Overfitting testing option 2 (MH) - note: I don't think CV_RMSE is needed, I think we want to test on the final model "best_xgb"
pred_train <- predict(best_xgb, dtrain)
pred_test  <- predict(best_xgb, dtest)

rmse_train <- rmse(train_data$avg_chip_seconds, pred_train)
rmse_test  <- rmse(test_data$avg_chip_seconds, pred_test)

c(Train_RMSE = rmse_train, Test_RMSE = rmse_test)

#Visually inspect for overfitting with learning curves plotting the nround
cv_results <- as.data.frame(xgb_cv$evaluation_log)

ggplot(cv_results, aes(x = iter)) +
  geom_line(aes(y = train_rmse_mean, color = "Train")) +
  geom_line(aes(y = test_rmse_mean, color = "Validation")) +
  labs(y = "RMSE", x = "nround", color = "Dataset")

```

MH: The learning curve and the compariosn of RMSE on test and training data with the best_xgb model suggests overfitting of data.


- Initial attempt end






# XGBoost model with no feature engineering or binning: (KB)
Tree-based models naturally capture nonlinearities and interactions without requiring us to create them. This could help us validate whether the interactions we hand-selected align with the patterns the model finds.

We will use the preprocessed dataset (cleaned, imputed, encoded. However, we will remove engineered interactions and bins when training XGBoost.

Import data (KB)
```{r}
final_data <- read.csv(here("data", "merged_marathon_data.csv"))
```

Spit the data so Berlin is used as a second case study to show how the method performs with missing data (whether successful or not). (KB)
```{r}
main_data <- final_data %>% filter(marathon != "Berlin")
berlin_data <- final_data %>% filter(marathon == "Berlin")

str(main_data)
```
We can see the main_data with out berlin now has 770 obs. and 21 variables. 

Since PM10 has a lot of missing values still, we will drop that column completely: (KB)
```{r}
main_data <- main_data %>%
  select(-pm10) 
```


Since PM2.5 is often the main pollutant, we decided to use KNN imputation to fill missing PM2.5 values because it predicts missing data using similar rows without assuming a specific parametric relationship, preserves variance, and works well for our relatively small dataset while using correlations with other environmental variables: (KB)
```{r}
# Impute missing PM2.5 values using 5 nearest neighbors
main_data <- kNN(main_data, variable = "pm25", k = 5) # can change later to see which K gives best model performance 

# remove pm25_imp
cols_to_remove <- c(
  "pm25_imp"
)

main_data <- main_data[, !(names(main_data) %in% cols_to_remove)]

# Check that missing values are filled
summary(main_data$pm25)
```
We can see that there are no missing values now and we can get a clean summary of the data. 

Convert categorical variables to factors: (KB)
```{r}
main_data  <- main_data  %>%
  mutate(subgroup = factor(subgroup),
         gender = factor(gender),
         marathon = factor(marathon),
         main_pollutant = factor(main_pollutant))

str(main_data )
```
We can see that the identifiers (subgroup, gender, marathon) and predictor (main_pollutant) were all converted to factors so that our models can properly recognized them. 

Add our 90/10 training/test split (ZD, KB)
```{r}
# split data into train and test
set.seed(123)

train_index <- sample(1:nrow(main_data), size = 0.9 * nrow(main_data)) # use a 90/10 split

train_data_raw <- as.data.frame(main_data[train_index, ]) # making sure they are data frames
test_data_raw  <- as.data.frame(main_data[-train_index, ])
```

(ZD, KB)
```{r}
# Remove identifiers before creating model matrix
# Convert dataset to numeric-only matrix
train_matrix_raw <- model.matrix(
  avg_chip_seconds ~ .,
  data = train_data_raw %>% select(-year, -n)
)

test_matrix_raw <- model.matrix(
  avg_chip_seconds ~ .,
  data = test_data_raw %>% select(-year, -n)
)

# Convert to DMatrix format for tuning, needed for cross validation
dtrain_raw <- xgb.DMatrix(data = train_matrix_raw, label = train_data_raw$avg_chip_seconds)
dtest_raw  <- xgb.DMatrix(data = test_matrix_raw,  label = test_data_raw$avg_chip_seconds)
```

Creating the XGBoost Model(ZD, KB) 
```{r}
# Create an untuned XGBoost model
xgb_base_raw <- xgboost(
  data = dtrain_raw,
  nrounds = 200,
  objective = "reg:squarederror",
  verbose = FALSE # mute iterations
)

pred_xgb_raw <- predict(xgb_base_raw, dtest_raw)

rmse_xgb_raw <- rmse(test_data_raw$avg_chip_seconds, pred_xgb_raw)
mae_xgb_raw  <- mae(test_data_raw$avg_chip_seconds, pred_xgb_raw)

c(RMSE = rmse_xgb_raw, MAE = mae_xgb_raw)
```
We can see that without manual feature engineering or creating binned variables (where the model showed previously RMSE: 131.9877 and MAE: 101.7843),the XGBoost model achieves lower RMSE and MAE. This suggests that XGBoost is effectively capturing nonlinearities and interactions on its own, reducing the need for us to manually create them.

Introduce cross validation hyperparameter tuning (ZD, KB)
```{r}
set.seed(123)  # for reproducibility

xgb_cv_raw <- xgb.cv(
  data = dtrain_raw,
  nrounds = 5000,      # Reduced nrounds from 2000 to speed up training while still allowing enough trees
  eta = 0.05,          # Increased eta from 0.01 to allow faster learning and reduce the number of trees needed
  nfold = 5,
  objective = "reg:squarederror",
  max_depth = 4,
  subsample = 0.8,
  colsample_bytree = 0.8,
  early_stopping_rounds = 50,
  verbose = 0, # mute iterations
  seed = 123  # fixes XGBoost internal randomness
)

xgb_cv_raw$best_iteration
```
We can see that the XGBoost model performed best at 4124 trees according to cross-validation. 
Adding more trees didn’t improve validation error.

Training the tunned model: (ZD, KB)
```{r}
# Train tuned model
params_raw <- list(
  objective = "reg:squarederror",
  eta = 0.05,          # matched the eta used in CV
  max_depth = 4,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Find best # of trees from cv
best_xgb_raw <- xgb.train(
  params = params_raw,
  data = dtrain_raw,
  nrounds = xgb_cv_raw$best_iteration,  # 4124 
  verbose = 0
)
```

Next we will evaluate the tuned model on the test set (ZD, KB)
```{r}
# Evaluate tuned model
pred_best_raw <- predict(best_xgb_raw, dtest_raw)

rmse_best_raw <- rmse(test_data_raw$avg_chip_seconds, pred_best_raw)
mae_best_raw  <- mae(test_data_raw$avg_chip_seconds, pred_best_raw)

c(RMSE = rmse_best_raw, MAE = mae_best_raw)

```

We can see the same trend on the test set (where the feature engineered and binned GXBoost showed RMSE: 128.51255 and MAE: 98.61524). This XGBoost model achieves lower RMSE and MAE. This suggests that XGBoost is effectively capturing nonlinearities and interactions on its own, reducing the need for us to manually create them.

Find Variable importance amongst XGBoost model, so we can see how it compares to the GXBoost that incudes feature enginnering and binning (ZD, KB)
```{r}
# Variable importance
importance_raw <- xgb.importance(model = best_xgb_raw)
print(importance_raw)

xgb.plot.importance(importance_raw)
```
We can see that subgroupslow and subgroupelite are the features the model relied on most to reduce prediction error. Variables like wind_speed or low_temp are less influential but still contribute a little. Features with extremely low Gain and Frequency, like main_pollutantPM10, are barely used by the model. 


Check for overfitting (ZD, KB)
```{r}
# Overfitting?
cv_rmse_raw <- min(xgb_cv_raw$evaluation_log$test_rmse_mean)
train_rmse_raw <- min(xgb_cv_raw$evaluation_log$train_rmse_mean)

data.frame(
  Train_RMSE = train_rmse_raw,
  CV_RMSE    = cv_rmse_raw,
  Test_RMSE  = rmse_best_raw
)

```

```{r}
#Overfitting testing option 2 (MH) - Testing overfitting on final model, calculating RMSE. The result/interpretation is effectively the same 
pred_train <- predict(best_xgb_raw, dtrain_raw)
pred_test  <- predict(best_xgb_raw, dtest_raw)

rmse_train <- rmse(train_data_raw$avg_chip_seconds, pred_train)
rmse_test  <- rmse(test_data_raw$avg_chip_seconds, pred_test)

c(Train_RMSE = rmse_train, Test_RMSE = rmse_test)

#Visually inspect for overfitting with learning curves plotting RMSE against the nround
cv_results <- as.data.frame(xgb_cv_raw$evaluation_log)

ggplot(cv_results, aes(x = iter)) +
  geom_line(aes(y = train_rmse_mean, color = "Train")) +
  geom_line(aes(y = test_rmse_mean, color = "Validation")) +
  labs(y = "RMSE", x = "nrounds", color = "Dataset")
```


We can see that the model is overfitting: There is a huge gap between Train_RMSE (15.07) and CV_RMSE (159.69) which means the model is overfitting the training data.






#MH Investigating more iterations of Multiple Linear Regression from initial model
The following code are sections pulled form initial modeling rmd file to prep for additional multiple linear regression investigation

```{r}
main_data <- read.csv(here("data", "pre_feat_data.csv"))
#Make sure marathon, gender,subgroup, and supershoe are all factors (KB)
main_data$supershoe <- factor(main_data$supershoe)
main_data$marathon <- factor(main_data$marathon)
main_data$gender   <- factor(main_data$gender)
main_data$subgroup <- factor(main_data$subgroup)

#split data into train and test
set.seed(123)

train_index <- sample(1:nrow(main_data), size = 0.9 * nrow(main_data)) # use a 90/10 split

train_data <- main_data[train_index, ]
test_data  <- main_data[-train_index, ]

# Identify numeric predictors to scale (exclude outcome and identifiers)
numeric_vars <- train_data %>%
  select(where(is.numeric)) %>%
  select(-avg_chip_seconds, -year, -n) %>%
  names()

# scale training data and save scaling parameters
train_scaled <- scale(train_data[numeric_vars])
train_data[paste0("scaled_", numeric_vars)] <- train_scaled

train_center <- attr(train_scaled, "scaled:center")
train_scale  <- attr(train_scaled, "scaled:scale")

# scale test data using training parameters
test_scaled <- scale(test_data[numeric_vars],
                     center = train_center,
                     scale  = train_scale)
test_data[paste0("scaled_", numeric_vars)] <- test_scaled


summary(train_data[paste0("scaled_", numeric_vars)])

# Adding Binned features based off Decision Tree (ozone_bin and pm25_bin) (KB)

# Training data 
train_data$ozone_bin <- factor(ifelse(train_data$ozone >= 38, 1, 0), levels = c(0, 1))
train_data$pm25_bin  <- factor(ifelse(train_data$pm25 >= 54, 1, 0), levels = c(0, 1))

# Test data using same thresholds
test_data$ozone_bin <- factor(ifelse(test_data$ozone >= 38, 1, 0), levels = c(0, 1))
test_data$pm25_bin  <- factor(ifelse(test_data$pm25 >= 54, 1, 0), levels = c(0, 1))

# Remove scaled_ozone from training and test data - from LASSO results in initial model (KB)
train_data <- train_data[, !(names(train_data) %in% c("scaled_ozone"))]
test_data  <- test_data[, !(names(test_data) %in% c("scaled_ozone"))]

```

#In the initial model report, we said we would
Based on our current analysis, we may refine the feature set by including only the variables with clear signals to help reduce noise and improve interpretability through methods like forward, backward and stepwise selection. We also plan on testing a model with the original continuous features rather than the scaled features. 

We also plan to run a few models testing the model with and on the Berlin data as a case study. 

#Model with no scaled features
```{r}
lm_initial_PM25 <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + avg_temp + precipitation + dew_point + wind_speed + visibility + sea_level_pressure + co + pm25 + ozone_bin + no2 + temp_aqi_interaction + avg_temp_gender_interaction, data = train_data)


summary(lm_initial_PM25)
plot(lm_initial_PM25)

lm_initial_PM25bin <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + avg_temp + precipitation + dew_point + wind_speed + visibility + sea_level_pressure + co + pm25_bin + ozone_bin + no2 + temp_aqi_interaction + avg_temp_gender_interaction, data = train_data)


summary(lm_initial_PM25bin)
plot(lm_initial_PM25bin)

```

```{r}
#Adding RMSE and MAE for looking at overfitting with test results below (MH/ZD)
residuals_lm_initial_PM25bin <- lm_initial_PM25bin$residuals
residuals_lm_initial_PM25bin_rmse <- sqrt(mean(residuals_lm_initial_PM25bin^2))
residuals_lm_initial_PM25bin_mae <- mean(abs(residuals_lm_initial_PM25bin))

residuals_lm_initial_PM25 <- lm_initial_PM25$residuals
residuals_lm_initial_PM25_rmse <- sqrt(mean(residuals_lm_initial_PM25^2))
residuals_lm_initial_PM25_mae <- mean(abs(residuals_lm_initial_PM25))



model_compare_train <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE  = c(residuals_lm_initial_PM25_rmse,
            residuals_lm_initial_PM25bin_rmse),
  MAE   = c(residuals_lm_initial_PM25_mae,
            residuals_lm_initial_PM25bin_mae),
  R2    = c(summary(lm_initial_PM25)$r.squared,
            summary(lm_initial_PM25bin)$r.squared)
)

# Display nicely formatted table
knitr::kable(model_compare_train, digits = 4,
             caption = "Training Data Comparison: Continuous vs Binned PM2.5")
```

```{r}
# More Modeling (ZD)

# build a function
evaluate <- function(model, test_data) {
  preds <- predict(model, newdata = test_data)
  actual <- test_data$avg_chip_seconds
  
  rmse <- sqrt(mean((preds - actual)^2))
  mae  <- mean(abs(preds - actual))
  r2   <- cor(preds, actual)^2
  
  return(list(RMSE = rmse, MAE = mae, R2 = r2))
}

# Evaluate Model 1 with scaled PM2.5
results_scaledPM25 <- evaluate(lm_initial_PM25, test_data)

# Evaluate Model 2 with PM2.5 bin
results_PM25bin <- evaluate(lm_initial_PM25bin, test_data)

# compare results (ZD)
model_compare <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE = c(results_scaledPM25$RMSE, results_PM25bin$RMSE),
  MAE  = c(results_scaledPM25$MAE,  results_PM25bin$MAE),
  R2   = c(results_scaledPM25$R2,   results_PM25bin$R2)
)

knitr::kable(model_compare, digits = 4,
             caption = "Comparison of Initial Models: Continuous vs Binned PM2.5")
```


#Result:
Scaling makes no difference to the model


#Next:Forward Backward Stepwise Selection