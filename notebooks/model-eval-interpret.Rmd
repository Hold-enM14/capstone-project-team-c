---
title: "Model Evaluation and Interpretation"
author: "Team C"
date: "2025-12-04"
output: html_document
---

# Load libraries
```{r}
library(here)
library(dplyr)
library(knitr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(VIM)
library(combinat)
library(xgboost)
library(Metrics)
#library(SHAPforxgboost)
```

# Importing the data (KB)
```{r}
main_data <- read.csv(here("data", "pre_feat_data.csv"))
```

Make sure marathon, gender,subgroup, and supershoe are all factors (KB)
```{r}
main_data$supershoe <- factor(main_data$supershoe)
main_data$marathon <- factor(main_data$marathon)
main_data$gender   <- factor(main_data$gender)
main_data$subgroup <- factor(main_data$subgroup)

str(main_data)
```
Now all characters are factors ready for modeling. 

# Add our 90/10 training/test split (ZD)
```{r}
# split data into train and test
set.seed(123)

train_index <- sample(1:nrow(main_data), size = 0.9 * nrow(main_data)) # use a 90/10 split

train_data <- main_data[train_index, ]
test_data  <- main_data[-train_index, ]
```


# Data Scaling (for linear regression model): (ZD, KB)

```{r}
# Identify numeric predictors to scale (exclude outcome and identifiers)
numeric_vars <- train_data %>%
  select(where(is.numeric)) %>%
  select(-avg_chip_seconds, -year, -n) %>%
  names()

# scale training data and save scaling parameters
train_scaled <- scale(train_data[numeric_vars])
train_data[paste0("scaled_", numeric_vars)] <- train_scaled

train_center <- attr(train_scaled, "scaled:center")
train_scale  <- attr(train_scaled, "scaled:scale")

# scale test data using training parameters
test_scaled <- scale(test_data[numeric_vars],
                     center = train_center,
                     scale  = train_scale)
test_data[paste0("scaled_", numeric_vars)] <- test_scaled


summary(train_data[paste0("scaled_", numeric_vars)])

```


```{r}
summary(test_data[paste0("scaled_", numeric_vars)])
```


We can see that all the continuous variables for the train and test data, not including the identifying variables `year` and `n`, were successfully scaled with means at 0 and standard deviations of 1, showing that the standardization was correctly applied.

# Adding Binned features based off Decision Tree (ozone_bin and pm25_bin) (KB)
```{r}
# Training data 
train_data$ozone_bin <- factor(ifelse(train_data$ozone >= 38, 1, 0), levels = c(0, 1))
train_data$pm25_bin  <- factor(ifelse(train_data$pm25 >= 54, 1, 0), levels = c(0, 1))

# Test data using same thresholds
test_data$ozone_bin <- factor(ifelse(test_data$ozone >= 38, 1, 0), levels = c(0, 1))
test_data$pm25_bin  <- factor(ifelse(test_data$pm25 >= 54, 1, 0), levels = c(0, 1))
```

We made the factor levels:  
- ozone bin: 0 = low, 1 = high
- pm25 bin: 0 = low, 1 = high


```{r}
str(train_data)
```



# LASSO is being used to answer the following question: (KB)

Which version of the air-quality and temperature features should we keep for the final model?
```{r}
library(glmnet)

# Prepare the predictor matrix (x) and outcome vector (y)
# Only using ozone and PM2.5 (continuous and binned)
x_train <- model.matrix(avg_chip_seconds ~ scaled_ozone + scaled_pm25 + 
                        ozone_bin + pm25_bin, data = train_data)[,-1]  # remove intercept
y_train <- train_data$avg_chip_seconds

x_test <- model.matrix(avg_chip_seconds ~ scaled_ozone + scaled_pm25 + 
                       ozone_bin + pm25_bin, data = test_data)[,-1]
y_test <- test_data$avg_chip_seconds

# Run LASSO with cross-validation to select lambda
set.seed(123)
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, standardize = FALSE)  # already scaled

# Find the best lambda
best_lambda <- lasso_cv$lambda.min
best_lambda

# Fit final LASSO model at best lambda
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda, standardize = FALSE)

# Check coefficients
coef(lasso_model)

# Predict on test set
preds <- predict(lasso_model, newx = x_test)

# Evaluate performance
rmse <- sqrt(mean((y_test - preds)^2))
mae  <- mean(abs(y_test - preds))
rmse
mae

```


We can see that both binned features and scaled_pm25 were retained showing (.). We can also see that scaled_ozone shrank to 0, which means it did not contribute to the predicting average chip time once the binned values were also included. The model achieved an RMSE of 3383 seconds and a MAE of 2861 seconds, suggesting that good predictive performance. 

Removing ozone based on LASSO results (KB)
```{r}
# Remove scaled_ozone from training and test data
train_data <- train_data[, !(names(train_data) %in% c("scaled_ozone"))]
test_data  <- test_data[, !(names(test_data) %in% c("scaled_ozone"))]

str(train_data)
str(test_data)
```

We can see that scaled ozone was successfully removed from both train and test data. 


```{r}
#Initial Model (MH)
lm_initial_scaledPM25 <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure + scaled_co + scaled_pm25 + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = train_data)


summary(lm_initial_scaledPM25)
plot(lm_initial_scaledPM25)

lm_initial_PM25bin <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure + scaled_co + pm25_bin + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = train_data)


summary(lm_initial_PM25bin)
plot(lm_initial_PM25bin)

```

```{r}
#Adding RMSE and MAE for looking at overfitting with test results below (MH/ZD)
residuals_lm_initial_PM25bin <- lm_initial_PM25bin$residuals
residuals_lm_initial_PM25bin_rmse <- sqrt(mean(residuals_lm_initial_PM25bin^2))
residuals_lm_initial_PM25bin_mae <- mean(abs(residuals_lm_initial_PM25bin))

residuals_lm_initial_scaledPM25 <- lm_initial_scaledPM25$residuals
residuals_lm_initial_scaledPM25_rmse <- sqrt(mean(residuals_lm_initial_scaledPM25^2))
residuals_lm_initial_scaledPM25_mae <- mean(abs(residuals_lm_initial_scaledPM25))



model_compare_train <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE  = c(residuals_lm_initial_scaledPM25_rmse,
            residuals_lm_initial_PM25bin_rmse),
  MAE   = c(residuals_lm_initial_scaledPM25_mae,
            residuals_lm_initial_PM25bin_mae),
  R2    = c(summary(lm_initial_scaledPM25)$r.squared,
            summary(lm_initial_PM25bin)$r.squared)
)

# Display nicely formatted table
knitr::kable(model_compare_train, digits = 4,
             caption = "Training Data Comparison: Continuous vs Binned PM2.5")

```





```{r}
# More Modeling (ZD)

# build a function
evaluate <- function(model, test_data) {
  preds <- predict(model, newdata = test_data)
  actual <- test_data$avg_chip_seconds
  
  rmse <- sqrt(mean((preds - actual)^2))
  mae  <- mean(abs(preds - actual))
  r2   <- cor(preds, actual)^2
  
  return(list(RMSE = rmse, MAE = mae, R2 = r2))
}

# Evaluate Model 1 with scaled PM2.5
results_scaledPM25 <- evaluate(lm_initial_scaledPM25, test_data)

# Evaluate Model 2 with PM2.5 bin
results_PM25bin <- evaluate(lm_initial_PM25bin, test_data)
```

```{r}
# compare results (ZD)
model_compare <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE = c(results_scaledPM25$RMSE, results_PM25bin$RMSE),
  MAE  = c(results_scaledPM25$MAE,  results_PM25bin$MAE),
  R2   = c(results_scaledPM25$R2,   results_PM25bin$R2)
)

knitr::kable(model_compare, digits = 4,
             caption = "Comparison of Initial Models: Continuous vs Binned PM2.5")
```

```{r}
# build residual plots (ZD)
plot(lm_initial_PM25bin, which = 1)   # Residuals vs Fitted
plot(lm_initial_PM25bin, which = 2)   # Q–Q plot
```


# Start of Model Evaluation and Interpretation section:
Attempt for XGboost model containing the engineered features, binned features, and scaled features: (ZD)

```{r}
# Remove identifiers before creating model matrix
# Convert dataset to numeric-only matrix
train_matrix <- model.matrix(
  avg_chip_seconds ~ .,
  data = train_data %>% select(-year, -n)
)

test_matrix <- model.matrix(
  avg_chip_seconds ~ .,
  data = test_data %>% select(-year, -n)
)

# Convert to DMatrix format for tuning, needed for cross validation
dtrain <- xgb.DMatrix(data = train_matrix, label = train_data$avg_chip_seconds)
dtest  <- xgb.DMatrix(data = test_matrix,  label = test_data$avg_chip_seconds)
```

(ZD)
```{r}
# Create an untuned XGBoost model
xgb_base <- xgboost(
  data = dtrain,
  nrounds = 200,
  objective = "reg:squarederror",
  verbose = FALSE # mute iterations
)

pred_xgb <- predict(xgb_base, dtest)

rmse_xgb <- rmse(test_data$avg_chip_seconds, pred_xgb)
mae_xgb  <- mae(test_data$avg_chip_seconds, pred_xgb)

c(RMSE = rmse_xgb, MAE = mae_xgb)
```

This represents the initial overfitting model prior to tuning (ZD, KB)
```{r}
# Zack's original attempt

xgb_cv_of <- xgb.cv(
  data = dtrain,
  nrounds = 2000,
  nfold = 5,
  objective = "reg:squarederror",
  eta = 0.01,
  max_depth = 4,
  subsample = 0.8,
  colsample_bytree = 0.8,
  early_stopping_rounds = 50,
  verbose = 0 # mute iterations
)

xgb_cv_of$best_iteration

# Train model
params_of <- list(
  objective = "reg:squarederror",
    eta = 0.01,
    max_depth = 4,
    subsample = 0.8,
    colsample_bytree = 0.8
)
# Find best # of trees from cv
best_xgb_of <- xgb.train(
  params = params_of,
  data = dtrain,
  nrounds = xgb_cv_of$best_iteration #2000
)
```
We can see that the XGBoost model performed best at 2000 trees according to cross-validation, with is hitting the max nrounds. 

Checking to see if the model is overfitting (ZD, KB)
```{r}
# Overfitting?
cv_rmse_of <- min(xgb_cv_of$evaluation_log$test_rmse_mean)
train_rmse_of <- min(xgb_cv_of$evaluation_log$train_rmse_mean)

data.frame(
  Train_RMSE = train_rmse_of,
  CV_RMSE = cv_rmse_of,
  Test_RMSE = rmse_xgb
)
```
We can see that the model is strongly overfitting because the train error (73) is alot lower than CV (188).

Introduce cross validation hyperparameter tuning (ZD, KB)
```{r}
# Krisha's attempt tuned for overfitting
set.seed(123)  # for reproducibility

xgb_cv <- xgb.cv(
  data = dtrain,
  nrounds = 3000,           # Increased to 3000 because 
  eta = 0.1,                # Increased to 0.1 because a higher eta helps overfitting when depth is shallow.
  max_depth = 2,            # Lowered to 2 because shallow trees reduce model complexity and prevent overfitting.
  min_child_weight = 10,    # Added to help model focus on stronger patterns instead of noise.
  nfold = 5,
  objective = "reg:squarederror",
  subsample = 0.8,          
  colsample_bytree = 0.7,   # Lowered to 0.7 to reduce the number of features considered per tree,reducing reliance on any single predictor.
  gamma = 0.1,              # Added to require a minimum loss reduction before splitting to limit noisy patterns.
  lambda = 1,               # L2 regularization (ridge penalty) stabilizes coefficients and reduces overfitting.
  alpha = 1,                # L1 regularization (lasso penalty) helps by shrinking weak predictors’
  early_stopping_rounds = 50,
  verbose = 0
)

xgb_cv$best_iteration
```
We can see that the XGBoost model performed best at 2182 trees according to cross-validation. 

Training the tunned model: (ZD, KB)
```{r}
# Train tuned model
params <- list(
  objective = "reg:squarederror",
  eta = 0.1,
  max_depth = 2,
  subsample = 0.8,
  colsample_bytree = 0.7, 
  gamma = 0.1, # 
  lambda = 1,
  alpha = 1
  
)
# Find best # of trees from cv
best_xgb <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = xgb_cv$best_iteration #2182
)
```


Next we will evaluate the tuned model on the test set (ZD, KB)
```{r}
#Calculating R2
rss <- sum((test_data$avg_chip_seconds - pred_xgb)^2)
tss <- sum((test_data$avg_chip_seconds - mean(test_data$avg_chip_seconds))^2)
r2_xgb <- 1 - rss/tss

# XGBoost model comparison (ZD, KB)
model_compare <- data.frame(
  Model = c("XGBoost (with engineering)"),
  RMSE = c(rmse_xgb),
  MAE  = c(mae_xgb),
  R2   = c(r2_xgb)
)

knitr::kable(model_compare, digits = 4,
             caption = "XGBoost (with engineering)")
```

We can see that the RMSE and MAE are alot lower comapred to our baseline models, where Continuous PM2.5	RMSE :357.0011	and MAE: 267.278, and PM2.5 Bin	RMSE: 358.0687	and MAE:274.0892. 


```{r}
# Variable importance amongst XGBoosted model
importance <- xgb.importance(model = best_xgb)
print(importance)

xgb.plot.importance(importance)
```

```{r}
# Overfitting?
cv_rmse <- min(xgb_cv$evaluation_log$test_rmse_mean)
train_rmse <- min(xgb_cv$evaluation_log$train_rmse_mean)

data.frame(
  Train_RMSE = train_rmse,
  CV_RMSE = cv_rmse,
  Test_RMSE = rmse_xgb
)
```

# XGBoost model with no feature engineering or binning: (KB)
Tree-based models naturally capture nonlinearities and interactions without requiring us to create them. This could help us validate whether the interactions we hand-selected align with the patterns the model finds.

We will use the preprocessed dataset (cleaned, imputed, encoded. However, we will remove engineered interactions and bins when training XGBoost.

Import data (KB)
```{r}
final_data <- read.csv(here("data", "merged_marathon_data.csv"))
```

Split the data so Berlin is used as a second case study to show how the method performs with missing data (whether successful or not). (KB)
```{r}
main_data <- final_data %>% filter(marathon != "Berlin")
berlin_data <- final_data %>% filter(marathon == "Berlin")

str(main_data)
```
We can see the main_data with out berlin now has 770 obs. and 21 variables. 

Since PM10 has a lot of missing values still, we will drop that column completely: (KB)
```{r}
main_data <- main_data %>%
  select(-pm10) 
```


Since PM2.5 is often the main pollutant, we decided to use KNN imputation to fill missing PM2.5 values because it predicts missing data using similar rows without assuming a specific parametric relationship, preserves variance, and works well for our relatively small dataset while using correlations with other environmental variables: (KB)
```{r}
# Impute missing PM2.5 values using 5 nearest neighbors
main_data <- kNN(main_data, variable = "pm25", k = 5) # can change later to see which K gives best model performance 

# remove pm25_imp
cols_to_remove <- c(
  "pm25_imp"
)

main_data <- main_data[, !(names(main_data) %in% cols_to_remove)]

# Check that missing values are filled
summary(main_data$pm25)
```
We can see that there are no missing values now and we can get a clean summary of the data. 

Convert categorical variables to factors: (KB)
```{r}
main_data  <- main_data  %>%
  mutate(subgroup = factor(subgroup),
         gender = factor(gender),
         marathon = factor(marathon),
         main_pollutant = factor(main_pollutant))

str(main_data )
```
We can see that the identifiers (subgroup, gender, marathon) and predictor (main_pollutant) were all converted to factors so that our models can properly recognized them. 

Add our 90/10 training/test split (ZD, KB)
```{r}
# split data into train and test
set.seed(123)

train_index <- sample(1:nrow(main_data), size = 0.9 * nrow(main_data)) # use a 90/10 split

train_data_raw <- as.data.frame(main_data[train_index, ]) # making sure they are data frames
test_data_raw  <- as.data.frame(main_data[-train_index, ])
```

(ZD, KB)
```{r}
# Remove identifiers before creating model matrix
# Convert dataset to numeric-only matrix
train_matrix_raw <- model.matrix(
  avg_chip_seconds ~ .,
  data = train_data_raw %>% select(-year, -n)
)

test_matrix_raw <- model.matrix(
  avg_chip_seconds ~ .,
  data = test_data_raw %>% select(-year, -n)
)

# Convert to DMatrix format for tuning, needed for cross validation
dtrain_raw <- xgb.DMatrix(data = train_matrix_raw, label = train_data_raw$avg_chip_seconds)
dtest_raw  <- xgb.DMatrix(data = test_matrix_raw,  label = test_data_raw$avg_chip_seconds)
```

Creating the XGBoost Model(ZD, KB) 
```{r}
# Create an untuned XGBoost model
xgb_base_raw <- xgboost(
  data = dtrain_raw,
  nrounds = 200,
  objective = "reg:squarederror",
  verbose = FALSE # mute iterations
)

pred_xgb_raw <- predict(xgb_base_raw, dtest_raw)

rmse_xgb_raw <- rmse(test_data_raw$avg_chip_seconds, pred_xgb_raw)
mae_xgb_raw  <- mae(test_data_raw$avg_chip_seconds, pred_xgb_raw)

c(RMSE = rmse_xgb_raw, MAE = mae_xgb_raw)
```
We can see that without manual feature engineering or creating binned variables (where the model showed previously RMSE: 131.9877 and MAE: 101.7843),the XGBoost model achieves lower RMSE and MAE. This suggests that XGBoost is effectively capturing nonlinearities and interactions on its own, reducing the need for us to manually create them.

This represents the initial overfitting model prior to tuning (ZD, KB)
```{r}
# Zack's original attempt

xgb_cv_raw_of <- xgb.cv(
  data = dtrain_raw,
  nrounds = 2000,
  nfold = 5,
  objective = "reg:squarederror",
  eta = 0.01,
  max_depth = 4,
  subsample = 0.8,
  colsample_bytree = 0.8,
  early_stopping_rounds = 50,
  verbose = 0 # mute iterations
)

xgb_cv_raw_of$best_iteration

# Train model
params_raw_of <- list(
  objective = "reg:squarederror",
  eta = 0.01,
  max_depth = 4,
  subsample = 0.8,
  colsample_bytree = 0.8
)
# Find best # of trees from cv
best_xgb_raw_of <- xgb.train(
  params = params_raw_of,
  data = dtrain_raw,
  nrounds = xgb_cv_raw_of$best_iteration 
)
```
We can see that the XGBoost model performed best at 2000 trees according to cross-validation, with is hitting the max nrounds. 

Checking to see if the model is overfitting (ZD, KB)
```{r}
# Overfitting?
cv_rmse_raw_of <- min(xgb_cv_raw_of$evaluation_log$test_rmse_mean)
train_rmse_raw_of <- min(xgb_cv_raw_of$evaluation_log$train_rmse_mean)

data.frame(
  Train_RMSE = train_rmse_raw_of,
  CV_RMSE = cv_rmse_raw_of,
  Test_RMSE = rmse_xgb_raw
)
```
We can see that the model is strongly overfitting because the train error (84) is a lot lower than CV (208).

Introduce cross validation hyperparameter tuning (ZD, KB)
```{r}
set.seed(123)  # for reproducibility

xgb_cv_raw <- xgb.cv(
  data = dtrain_raw,
  nrounds = 3000,      
  eta = 0.1,                # Increased to 0.1 because a higher eta helps overfitting when depth is shallow.
  max_depth = 2,            # Lowered to 2 because shallow trees reduce model complexity and prevent overfitting.
  min_child_weight = 10,    # Added to help model focus on stronger patterns instead of noise.
  nfold = 5,
  objective = "reg:squarederror",
  subsample = 0.8,          
  colsample_bytree = 0.7,   # Lowered to 0.7 to reduce the number of features considered per tree,reducing reliance on any single predictor.
  gamma = 0.1,              # Added to require a minimum loss reduction before splitting to limit noisy patterns.
  lambda = 1,               # L2 regularization (ridge penalty) stabilizes coefficients and reduces overfitting.
  alpha = 1,                # L1 regularization (lasso penalty) helps by shrinking weak predictors’
  early_stopping_rounds = 50,
  verbose = 0
)

xgb_cv_raw$best_iteration
```
We can see that the XGBoost model performed best at 1353 trees according to cross-validation. 

Training the tunned model: (ZD, KB)
```{r}
# Train tuned model
params_raw <- list(
  objective = "reg:squarederror",
  eta = 0.1,          # matched the eta used in CV
  max_depth = 2,
  subsample = 0.8,
  colsample_bytree = 0.7,
  gamma = 0.1,
  lambda = 1,
  alpha = 1
)

# Find best # of trees from cv
best_xgb_raw <- xgb.train(
  params = params_raw,
  data = dtrain_raw,
  nrounds = xgb_cv_raw$best_iteration,  #  1353
  verbose = 0
)
```

Next we will evaluate the tuned model on the test set (ZD, KB)
```{r}
#Calculating R2
rss <- sum((test_data_raw$avg_chip_seconds - pred_xgb_raw)^2)
tss <- sum((test_data_raw$avg_chip_seconds - mean(test_data_raw$avg_chip_seconds))^2)
r2_xgb_raw <- 1 - rss/tss

# XGBoost model comparison (ZD, KB)
model_compare <- data.frame(
  Model = c("XGBoost (raw features)"),
  RMSE = c(rmse_xgb_raw),
  MAE  = c(mae_xgb_raw),
  R2   = c(r2_xgb_raw)
)

knitr::kable(model_compare, digits = 4,
             caption = " XGBoost (raw features)")
```

We can see the same trend on the test set (where XGBoost with feature engineering showed RMSE: 131.9877 and MAE: 101.7843). This XGBoost model achieves lower RMSE and MAE. This suggests that XGBoost is effectively capturing nonlinearities and interactions on its own, reducing the need for us to manually create them.

Find Variable importance amongst XGBoost model, so we can see how it compares to the XGBoost that incudes feature enginnering and binning (ZD, KB)
```{r}
# Variable importance
importance_raw <- xgb.importance(model = best_xgb_raw)
print(importance_raw)

xgb.plot.importance(importance_raw)
```
We can see that subgroupslow and subgroupelite are the features the model relied on most to reduce prediction error. Variables like wind_speed or low_temp are less influential but still contribute a little. Features with extremely low Frequency are main_pollutantPM2.5, which are barely used by the model. 


Check for overfitting (ZD, KB)
```{r}
# Overfitting?
cv_rmse_raw <- min(xgb_cv_raw$evaluation_log$test_rmse_mean)
train_rmse_raw <- min(xgb_cv_raw$evaluation_log$train_rmse_mean)

data.frame(
  Train_RMSE = train_rmse_raw,
  CV_RMSE    = cv_rmse_raw,
  Test_RMSE  = rmse_xgb_raw
)

```

(MH)

```{r}
eval_log <- xgb_cv_raw$evaluation_log

ggplot(eval_log, aes(x = iter)) +
geom_line(aes(y = train_rmse_mean, color = "Training")) +
geom_line(aes(y = test_rmse_mean, color = "Cross-validation")) +
labs(
title = "Learning Curve: Training vs Cross-validation",
x = "nrounds",
y = "RMSE"
) +
scale_color_manual(values = c("Training" = "blue", "Cross-validation" = "red")) +
theme_minimal()
```


We can now see that the model is no longer overfitting. 

Implement SHAP (ZD)

```{r}
# SHAP
library(SHAPforxgboost)   # we can move this library to the beginning after ensuring this chunk is needed
# Compute SHAP values for the raw XGB model
shap_values <- shap.values(
  xgb_model = best_xgb_raw,
  X_train = train_matrix_raw
)

shap_long <- shap.prep( # convert SHAP values to long format
  xgb_model = best_xgb_raw,
  X_train = train_matrix_raw
)
shap.plot.summary(shap_long)
```

SHAP values to left of 0 (negative) represent decreases in predicted finishing time, while SHAP values to the right of 0 (positive) represent increases in predicted finishing time.

Marathon location proves to be important in determining race results. Elite runners strongly decreased in predicted finishing time, while slow runners strongly increased in predicted finishing time. Temperature, ozone, dew point, and PM2.5 influence finishing time but their effects are limited compared to the subgrouping impacts.

(ZD/MH)

```{r}
# PM2.5 effect by subgroup
shap.plot.dependence(
  shap_long,
  x = "pm25",
  color_feature = "subgroup"
)

#Because we want to color by subgroup and our subgroups are one-hot encoded, the best way we found to do this is extrapolate for each one-hot encoded group and plot all together with ggplot

#plot for elite subgroup
elite <- shap.plot.dependence(
  shap_long,
  x = "pm25",
  color_feature = "subgroupelite"
)
#SHAP values broken down by 1 (elite) or 0 (not-elite) and relevant color.
elite_points <- elite$data

#repeated for competitive subgroup
comp <- shap.plot.dependence(
  shap_long,
  x = "pm25",
  color_feature = "subgroupcompetitive"
)
comp_points <- comp$data

#repeated for recreational subgroup
rec <- shap.plot.dependence(
  shap_long,
  x = "pm25",
  color_feature = "subgrouprecreational"
)
rec_points <- rec$data

#repeated for slow subgroup
slow <- shap.plot.dependence(
  shap_long,
  x = "pm25",
  color_feature = "subgroupslow"
)
slow_points <- slow$data



#subset so each dataset hase color_value = 1 (the dataframe's positive subroup)
elite_points <- subset(elite_points, color_value == 1)
comp_points <- subset(comp_points, color_value == 1)
rec_points <- subset(rec_points, color_value == 1)
slow_points <- subset(slow_points, color_value == 1)

#add group to each dataframe for merge
elite_points$group <- "Elite"
comp_points$group  <- "Comp"
rec_points$group   <- "Rec"
slow_points$group  <- "Slow"

#merge
all_points <- rbind(elite_points, comp_points, rec_points, slow_points)

#plot each subgroup and relevant SHAP values
ggplot(all_points, aes(x = x_feature, y = value, color = group)) +
  geom_point(alpha = 0.4) +
  geom_smooth(se = FALSE, size = 0.8) +   # one smooth line per group
  scale_color_manual(values = c(
    "Elite" = "#3B77BC",
    "Comp"  = "#E1BE6A",
    "Rec"   = "#55A868",
    "Slow"  = "#C44E52"
  ),
  breaks = c("Elite", "Comp", "Rec", "Slow") 
  ) +
  theme_minimal() +
  labs(x = "pm25", y = "SHAP value", color = "Group")

```

Low PM2.5 values equate to a minimal effect on finishing time. Moderate PM2.5 values display performance weakening. High PM2.5 values show that the model predicts faster times. Extremely high PM 2.5 values seem to lead to slower times for runners.

(ZD/MH)

```{r}
# Temperature effect by gender - 
shap.plot.dependence(
  shap_long,
  x = "avg_temp",
  color_feature = "gendermale"
)


#plot for gender:male 
gender <- shap.plot.dependence(
  shap_long,
  x = "avg_temp",
  color_feature = "gendermale"
)
#SHAP values broken down by 1 (male) or 0 (female) and relevant color.
gender_points <- gender$data

gender_points$color_value <- factor(
  gender_points$color_value,
  levels = c(0, 1),
  labels = c("Female", "Male")
)


#plot each subgroup and relevant SHAP values
ggplot(gender_points, aes(x = x_feature, y = value, color = color_value)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "loess", se = FALSE, size = 1.2) +
  scale_color_manual(
    values = c("Female" = "#E69F00", "Male" = "#0072B2")
  ) +
  labs(
    x = "Average Temp",
    y = "SHAP value",
    color = "Gender"
  ) +
  theme_minimal()

```

```{r}

#plot for elite subgroup
elite <- shap.plot.dependence(
  shap_long,
  x = "avg_temp",
  color_feature = "subgroupelite"
)
#SHAP values broken down by 1 (elite) or 0 (not-elite) and relevant color.
elite_points <- elite$data

#repeated for competitive subgroup
comp <- shap.plot.dependence(
  shap_long,
  x = "avg_temp",
  color_feature = "subgroupcompetitive"
)
comp_points <- comp$data

#repeated for recreational subgroup
rec <- shap.plot.dependence(
  shap_long,
  x = "avg_temp",
  color_feature = "subgrouprecreational"
)
rec_points <- rec$data

#repeated for slow subgroup
slow <- shap.plot.dependence(
  shap_long,
  x = "avg_temp",
  color_feature = "subgroupslow"
)
slow_points <- slow$data



#subset so each dataset hase color_value = 1 (the dataframe's positive subroup)
elite_points <- subset(elite_points, color_value == 1)
comp_points <- subset(comp_points, color_value == 1)
rec_points <- subset(rec_points, color_value == 1)
slow_points <- subset(slow_points, color_value == 1)

#add group to each dataframe for merge
elite_points$group <- "Elite"
comp_points$group  <- "Comp"
rec_points$group   <- "Rec"
slow_points$group  <- "Slow"

#merge
all_points <- rbind(elite_points, comp_points, rec_points, slow_points)

#plot each subgroup and relevant SHAP values
ggplot(all_points, aes(x = x_feature, y = value, color = group)) +
  geom_point(alpha = 0.4) +
  geom_smooth(se = FALSE, size = 0.8) +   # one smooth line per group
  scale_color_manual(values = c(
    "Elite" = "#3B77BC",
    "Comp"  = "#E1BE6A",
    "Rec"   = "#55A868",
    "Slow"  = "#C44E52"
  ),
  breaks = c("Elite", "Comp", "Rec", "Slow") 
  ) +
  theme_minimal() +
  labs(x = "Average Temp", y = "SHAP value", color = "Group")

```


```{r}
#plot for elite subgroup
elite <- shap.plot.dependence(
  shap_long,
  x = "ozone",
  color_feature = "subgroupelite"
)
#SHAP values broken down by 1 (elite) or 0 (not-elite) and relevant color.
elite_points <- elite$data

#repeated for competitive subgroup
comp <- shap.plot.dependence(
  shap_long,
  x = "ozone",
  color_feature = "subgroupcompetitive"
)
comp_points <- comp$data

#repeated for recreational subgroup
rec <- shap.plot.dependence(
  shap_long,
  x = "ozone",
  color_feature = "subgrouprecreational"
)
rec_points <- rec$data

#repeated for slow subgroup
slow <- shap.plot.dependence(
  shap_long,
  x = "ozone",
  color_feature = "subgroupslow"
)
slow_points <- slow$data



#subset so each dataset hase color_value = 1 (the dataframe's positive subroup)
elite_points <- subset(elite_points, color_value == 1)
comp_points <- subset(comp_points, color_value == 1)
rec_points <- subset(rec_points, color_value == 1)
slow_points <- subset(slow_points, color_value == 1)

#add group to each dataframe for merge
elite_points$group <- "Elite"
comp_points$group  <- "Comp"
rec_points$group   <- "Rec"
slow_points$group  <- "Slow"

#merge
all_points <- rbind(elite_points, comp_points, rec_points, slow_points)

#plot each subgroup and relevant SHAP values
ggplot(all_points, aes(x = x_feature, y = value, color = group)) +
  geom_point(alpha = 0.4) +
  geom_smooth(se = FALSE, size = 0.8) +   # one smooth line per group
  scale_color_manual(values = c(
    "Elite" = "#3B77BC",
    "Comp"  = "#E1BE6A",
    "Rec"   = "#55A868",
    "Slow"  = "#C44E52"
  ),
  breaks = c("Elite", "Comp", "Rec", "Slow") 
  ) +
  theme_minimal() +
  labs(x = "Ozone", y = "SHAP value", color = "Group")
```


Temperature displays a much clearer impact than the PM2.5 results. At lower temperatures, the SHAP values are negative and ultimately predict a faster finishing time. At moderate temperatures, the effects are rather neutral. At higher temperatures, the SHAP values begin to rise, indicating a slower predicted finish. While extremely warm temperatures the SHAP values sharply increase the predicted finishing time by a rough average of 400 seconds. Generally speaking, colder temperatures appear to improve race time while warmer temperatures hinder race time.

(ZD)

```{r}
shap_subgroup <- shap_long[grepl("subgroup", shap_long$variable), ]
shap.plot.summary(shap_subgroup)
```

The most important features appear to be the slow and elite subgroups, the male gender, and marathon locations. Being in the slow subgroup adds approximately (1510) seconds to the predicted finishing time. Belonging to the elite group decreases predicted finishing time by roughly (918) seconds. There appears to be some weight to the NYC and Chicago marathons which likely indicates a more challenging running route. 

(ZD)

```{r}
shap_gender <- shap_long[grepl("gender", shap_long$variable), ]
shap.plot.summary(shap_gender)
```

Here we can see that Male runners are able to consistently decrease the predicted finishing time, while Female runners consistently increase in predicted finishing time, making Gender an influential predictor of finishing time. 


Comparing all our models: (ZD, KB)
```{r}
model_compare_train <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin","XGBoost (with engineering)","XGBoost (raw features)"),
  RMSE  = c(results_scaledPM25$RMSE,
            results_PM25bin$RMSE, 
            rmse_xgb,
            rmse_xgb_raw),
  MAE   = c(results_scaledPM25$MAE,
            results_PM25bin$MAE,
            mae_xgb,
            mae_xgb_raw),
  R2    = c(results_scaledPM25$R2,
            results_PM25bin$R2,
            r2_xgb, 
            r2_xgb_raw)
)

# Display nicely formatted table
knitr::kable(model_compare_train, digits = 4,
             caption = "Comparison of All Models")

```

# Looking at our Predictions: (KB)

These results from the XGBoost (raw-features) model will help answer our predictions. 
```{r}
importance_raw <- xgb.importance(model = best_xgb_raw)
importance_raw
```

Answering the prediction: Moderate temperatures, low humidity, and clean air will produce the fastest finishing times across all categories (Gasparetto & Nesseler, 2020) (KB)
```{r}
#Starting by preparing the data
# Remove label
train_features <- train_data_raw %>% 
  select(-avg_chip_seconds)

# Convert all cat variables to numeric
cat_cols <- c("marathon", "gender", "subgroup", "main_pollutant")

train_features <- train_features %>%
  mutate(across(all_of(cat_cols), ~ as.numeric(as.factor(.))))

train_labels <- train_data_raw$avg_chip_seconds

# Function to predict xgb (using CHATgbt)
predict_xgb <- function(model, newdata) {
  newmat <- as.matrix(newdata)
  predict(model, newmat)
}

# Build explainer 
explainer_raw <- explain(
  model = best_xgb_raw,
  data = train_matrix_raw,
  y = train_labels,
  predict_function = predict_xgb,
  label = "XGB_raw"
)

# Partial dependence plots
pdp_temp <- partial_dependency(explainer_raw, variables = "avg_temp")
plot(pdp_temp)

pdp_pm25 <- partial_dependency(explainer_raw, variables = "pm25")
plot(pdp_pm25)

pdp_ozone <- partial_dependency(explainer_raw, variables = "ozone")
plot(pdp_ozone)

```

#Graphing gain, cover and frequency for best XGBoost (MH)

```{r}
# Variable importance
gain_fig_xgb <- xgb.plot.importance(importance_raw, measure ="Gain") 
cover_fig_xgb <- xgb.plot.importance(importance_raw, measure ="Cover") 
freq_fig_xgb <- xgb.plot.importance(importance_raw, measure ="Frequency")


#Used ai since I couldn't figure out how to add labels to the xgb.plot.importance() function
gain_fig <- ggplot(importance_raw, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Feature Importance by Gain",
    x = "Feature",
    y = "Importance Score"
  ) +
  theme_minimal()

print(gain_fig)


cover_fig <- ggplot(importance_raw, aes(x = reorder(Feature, Cover), y = Cover)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Feature Importance by Cover",
    x = "Feature",
    y = "Importance Score"
  ) +
  theme_minimal()

print(cover_fig)

 
freq_fig <- ggplot(importance_raw, aes(x = reorder(Feature, Cover), y = Frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Feature Importance by Frequency",
    x = "Feature",
    y = "Importance Score"
  ) +
  theme_minimal()

print(freq_fig)

```



#MH Investigating more iterations of Multiple Linear Regression from initial model
The following code are sections pulled form initial modeling rmd file to prep for additional multiple linear regression investigation

```{r}
main_data <- read.csv(here("data", "pre_feat_data.csv"))
#Make sure marathon, gender,subgroup, and supershoe are all factors (KB)
main_data$supershoe <- factor(main_data$supershoe)
main_data$marathon <- factor(main_data$marathon)
main_data$gender   <- factor(main_data$gender)
main_data$subgroup <- factor(main_data$subgroup)

#split data into train and test
set.seed(123)

train_index <- sample(1:nrow(main_data), size = 0.9 * nrow(main_data)) # use a 90/10 split

train_data <- main_data[train_index, ]
test_data  <- main_data[-train_index, ]

# Identify numeric predictors to scale (exclude outcome and identifiers)
numeric_vars <- train_data %>%
  select(where(is.numeric)) %>%
  select(-avg_chip_seconds, -year, -n) %>%
  names()

# scale training data and save scaling parameters
train_scaled <- scale(train_data[numeric_vars])
train_data[paste0("scaled_", numeric_vars)] <- train_scaled

train_center <- attr(train_scaled, "scaled:center")
train_scale  <- attr(train_scaled, "scaled:scale")

# scale test data using training parameters
test_scaled <- scale(test_data[numeric_vars],
                     center = train_center,
                     scale  = train_scale)
test_data[paste0("scaled_", numeric_vars)] <- test_scaled


summary(train_data[paste0("scaled_", numeric_vars)])

# Adding Binned features based off Decision Tree (ozone_bin and pm25_bin) (KB)

# Training data 
train_data$ozone_bin <- factor(ifelse(train_data$ozone >= 38, 1, 0), levels = c(0, 1))
train_data$pm25_bin  <- factor(ifelse(train_data$pm25 >= 54, 1, 0), levels = c(0, 1))

# Test data using same thresholds
test_data$ozone_bin <- factor(ifelse(test_data$ozone >= 38, 1, 0), levels = c(0, 1))
test_data$pm25_bin  <- factor(ifelse(test_data$pm25 >= 54, 1, 0), levels = c(0, 1))

# Remove scaled_ozone from training and test data - from LASSO results in initial model (KB)
train_data <- train_data[, !(names(train_data) %in% c("scaled_ozone"))]
test_data  <- test_data[, !(names(test_data) %in% c("scaled_ozone"))]

```

#In the initial model report, we said we would
Based on our current analysis, we may refine the feature set by including only the variables with clear signals to help reduce noise and improve interpretability through methods like forward, backward and stepwise selection. We also plan on testing a model with the original continuous features rather than the scaled features. 

We also plan to run a few models testing the model with and on the Berlin data as a case study. 

#Model with no scaled features
```{r}
lm_initial_PM25 <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + avg_temp + precipitation + dew_point + wind_speed + visibility + sea_level_pressure + co + pm25 + ozone_bin + no2 + temp_aqi_interaction + avg_temp_gender_interaction, data = train_data)


summary(lm_initial_PM25)
plot(lm_initial_PM25)

lm_initial_PM25bin <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + avg_temp + precipitation + dew_point + wind_speed + visibility + sea_level_pressure + co + pm25_bin + ozone_bin + no2 + temp_aqi_interaction + avg_temp_gender_interaction, data = train_data)


summary(lm_initial_PM25bin)
plot(lm_initial_PM25bin)

```

```{r}
#Adding RMSE and MAE for looking at overfitting with test results below (MH/ZD)
residuals_lm_initial_PM25bin <- lm_initial_PM25bin$residuals
residuals_lm_initial_PM25bin_rmse <- sqrt(mean(residuals_lm_initial_PM25bin^2))
residuals_lm_initial_PM25bin_mae <- mean(abs(residuals_lm_initial_PM25bin))

residuals_lm_initial_PM25 <- lm_initial_PM25$residuals
residuals_lm_initial_PM25_rmse <- sqrt(mean(residuals_lm_initial_PM25^2))
residuals_lm_initial_PM25_mae <- mean(abs(residuals_lm_initial_PM25))



model_compare_train <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE  = c(residuals_lm_initial_PM25_rmse,
            residuals_lm_initial_PM25bin_rmse),
  MAE   = c(residuals_lm_initial_PM25_mae,
            residuals_lm_initial_PM25bin_mae),
  R2    = c(summary(lm_initial_PM25)$r.squared,
            summary(lm_initial_PM25bin)$r.squared)
)

# Display nicely formatted table
knitr::kable(model_compare_train, digits = 4,
             caption = "Training Data Comparison: Continuous vs Binned PM2.5")
```

```{r}
# More Modeling (ZD)

# build a function
evaluate <- function(model, test_data) {
  preds <- predict(model, newdata = test_data)
  actual <- test_data$avg_chip_seconds
  
  rmse <- sqrt(mean((preds - actual)^2))
  mae  <- mean(abs(preds - actual))
  r2   <- cor(preds, actual)^2
  
  return(list(RMSE = rmse, MAE = mae, R2 = r2))
}

# Evaluate Model 1 with scaled PM2.5
results_scaledPM25 <- evaluate(lm_initial_PM25, test_data)

# Evaluate Model 2 with PM2.5 bin
results_PM25bin <- evaluate(lm_initial_PM25bin, test_data)

# compare results (ZD)
model_compare <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE = c(results_scaledPM25$RMSE, results_PM25bin$RMSE),
  MAE  = c(results_scaledPM25$MAE,  results_PM25bin$MAE),
  R2   = c(results_scaledPM25$R2,   results_PM25bin$R2)
)

knitr::kable(model_compare, digits = 4,
             caption = "Comparison of Initial Models: Continuous vs Binned PM2.5")
```


#Result:
Scaling makes no difference to the model


#Next:Stepwise Selection for initial model lm() with scaledPM2.5 (MH)
```{r}
both_initial_scaledPM25 <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure + scaled_co + scaled_pm25 + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = train_data)

both_initial_scaledPM25 <- step(both_initial_scaledPM25, direction ="both")

#removed scaled_avg_temp_gender_interaction based on stepwise model
both_results <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe +
              scaled_avg_temp + scaled_precipitation + scaled_dew_point +
              scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure +
              scaled_co + scaled_pm25 + ozone_bin + scaled_no2 +
              scaled_temp_aqi_interaction,
            data = train_data)


#Adding RMSE and MAE for looking at overfitting with test results below (MH/ZD)
residuals_lm_initial_PM25 <- both_results$residuals
residuals_lm_initial_PM25_rmse <- sqrt(mean(residuals_lm_initial_PM25^2))
residuals_lm_initial_PM25_mae <- mean(abs(residuals_lm_initial_PM25))


model_compare_train <- data.frame(
  Model = c("Continuous PM2.5"),
  RMSE  = c(residuals_lm_initial_PM25_rmse),
  MAE   = c(residuals_lm_initial_PM25_mae),
  R2    = c(summary(both_results)$r.squared)
)

# Display nicely formatted table
knitr::kable(model_compare_train, digits = 4,
             caption = "Training Data Comparison: Continuous vs Binned PM2.5")

# More Modeling (ZD)

# build a function
evaluate <- function(both_results, test_data) {
  preds <- predict(both_results, newdata = test_data)
  actual <- test_data$avg_chip_seconds
  
  rmse <- sqrt(mean((preds - actual)^2))
  mae  <- mean(abs(preds - actual))
  r2   <- cor(preds, actual)^2
  
  return(list(RMSE = rmse, MAE = mae, R2 = r2))
}

# Evaluate Model 1 with scaled PM2.5
results_scaledPM25 <- evaluate(both_results, test_data)


```
Stepwise selection did not improve upon initial model when removing scaled_avg_temp_gender_interaction

#Backward Stepwise
```{r}
back_initial_scaledPM25 <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure + scaled_co + scaled_pm25 + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = train_data)

back_initial_scaledPM25 <- step(back_initial_scaledPM25, direction ="backward")


#removes scaled_precipitation and scaled_precipitation                           

back_results <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe +
              scaled_avg_temp + scaled_precipitation + scaled_dew_point +
              scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure +
              scaled_co + scaled_pm25 + ozone_bin + scaled_no2 +
              scaled_temp_aqi_interaction,
            data = train_data)


#Adding RMSE and MAE for looking at overfitting with test results below (MH/ZD)
residuals_lm_initial_PM25 <- back_results$residuals
residuals_lm_initial_PM25_rmse <- sqrt(mean(residuals_lm_initial_PM25^2))
residuals_lm_initial_PM25_mae <- mean(abs(residuals_lm_initial_PM25))


model_compare_train <- data.frame(
  Model = c("Continuous PM2.5"),
  RMSE  = c(residuals_lm_initial_PM25_rmse),
  MAE   = c(residuals_lm_initial_PM25_mae),
  R2    = c(summary(back_results)$r.squared)
)

# Display nicely formatted table
knitr::kable(model_compare_train, digits = 4,
             caption = "Training Data Comparison: Continuous vs Binned PM2.5")

# More Modeling (ZD)

# build a function
evaluate <- function(back_results, test_data) {
  preds <- predict(back_results, newdata = test_data)
  actual <- test_data$avg_chip_seconds
  
  rmse <- sqrt(mean((preds - actual)^2))
  mae  <- mean(abs(preds - actual))
  r2   <- cor(preds, actual)^2
  
  return(list(RMSE = rmse, MAE = mae, R2 = r2))
}

# Evaluate Model 1 with scaled PM2.5
results_scaledPM25 <- evaluate(both_results, test_data)

```
Backward stepwise did not offer any additional clarity




# Code Dictionary
```Code we might need but didnt use```

```{r}
#Overfitting testing option 2 (MH) - note: I don't think CV_RMSE is needed, I think we want to test on the final model "best_xgb"
pred_train <- predict(best_xgb, dtrain)
pred_test  <- predict(best_xgb, dtest)

rmse_train <- rmse(train_data$avg_chip_seconds, pred_train)
rmse_test  <- rmse(test_data$avg_chip_seconds, pred_test)

c(Train_RMSE = rmse_train, Test_RMSE = rmse_test)

#Visually inspect for overfitting with learning curves plotting the nround
cv_results <- as.data.frame(xgb_cv$evaluation_log)

ggplot(cv_results, aes(x = iter)) +
  geom_line(aes(y = train_rmse_mean, color = "Train")) +
  geom_line(aes(y = test_rmse_mean, color = "Validation")) +
  labs(y = "RMSE", x = "nround", color = "Dataset")

```

MH: The learning curve and the compariosn of RMSE on test and training data with the best_xgb model suggests overfitting of data.


```{r}
#Overfitting testing option 2 (MH) - Testing overfitting on final model, calculating RMSE. The result/interpretation is effectively the same 
pred_train <- predict(best_xgb_raw, dtrain_raw)
pred_test  <- predict(best_xgb_raw, dtest_raw)

rmse_train <- rmse(train_data_raw$avg_chip_seconds, pred_train)
rmse_test  <- rmse(test_data_raw$avg_chip_seconds, pred_test)

c(Train_RMSE = rmse_train, Test_RMSE = rmse_test)

#Visually inspect for overfitting with learning curves plotting RMSE against the nround
cv_results <- as.data.frame(xgb_cv_raw$evaluation_log)

ggplot(cv_results, aes(x = iter)) +
  geom_line(aes(y = train_rmse_mean, color = "Train")) +
  geom_line(aes(y = test_rmse_mean, color = "Validation")) +
  labs(y = "RMSE", x = "nrounds", color = "Dataset")
```