---
title: "initialmodeling"
date: "2025-11-23"
output: html_document
---
# Load libraries
```{r}
library(here)
library(dplyr)
library(knitr)
library(tidyr)
library(ggplot2)
library(corrplot)
library(VIM)
library(combinat)
```

# Importing the data (KB)
```{r}
main_data <- read.csv(here("data", "pre_feat_data.csv"))
```

Make sure marathon, gender,subgroup, and supershoe are all factors (KB)
```{r}
main_data$supershoe <- factor(main_data$supershoe)
main_data$marathon <- factor(main_data$marathon)
main_data$gender   <- factor(main_data$gender)
main_data$subgroup <- factor(main_data$subgroup)

str(main_data)
```
Now all characters are factors ready for modeling. 

# Add our 90/10 training/test split (ZD)
```{r}
# split data into train and test
set.seed(123)

train_index <- sample(1:nrow(main_data), size = 0.9 * nrow(main_data)) # use a 90/10 split

train_data <- main_data[train_index, ]
test_data  <- main_data[-train_index, ]
```


# Data Scaling (for linear regression model): (ZD, KB)

```{r}
# Identify numeric predictors to scale (exclude outcome and identifiers)
numeric_vars <- train_data %>%
  select(where(is.numeric)) %>%
  select(-avg_chip_seconds, -year, -n) %>%
  names()

# scale training data and save scaling parameters
train_scaled <- scale(train_data[numeric_vars])
train_data[paste0("scaled_", numeric_vars)] <- train_scaled

train_center <- attr(train_scaled, "scaled:center")
train_scale  <- attr(train_scaled, "scaled:scale")

# scale test data using training parameters
test_scaled <- scale(test_data[numeric_vars],
                     center = train_center,
                     scale  = train_scale)
test_data[paste0("scaled_", numeric_vars)] <- test_scaled


summary(train_data[paste0("scaled_", numeric_vars)])

```


```{r}
summary(test_data[paste0("scaled_", numeric_vars)])
```


We can see that all the continuous variables for the train and test data, not including the identifying variables `year` and `n`, were successfully scaled with means at 0 and standard deviations of 1, showing that the standardization was correctly applied.

# Adding Binned features based off Decision Tree (ozone_bin and pm25_bin) (KB)
```{r}
# Training data 
train_data$ozone_bin <- factor(ifelse(train_data$ozone >= 38, 1, 0), levels = c(0, 1))
train_data$pm25_bin  <- factor(ifelse(train_data$pm25 >= 54, 1, 0), levels = c(0, 1))

# Test data using same thresholds
test_data$ozone_bin <- factor(ifelse(test_data$ozone >= 38, 1, 0), levels = c(0, 1))
test_data$pm25_bin  <- factor(ifelse(test_data$pm25 >= 54, 1, 0), levels = c(0, 1))
```

We made the factor levels:  
- ozone bin: 0 = low, 1 = high
- pm25 bin: 0 = low, 1 = high


```{r}
str(train_data)
```



# LASSO is being used to answer the following question: (KB)

Which version of the air-quality and temperature features should we keep for the final model?
```{r}
library(glmnet)

# Prepare the predictor matrix (x) and outcome vector (y)
# Only using ozone and PM2.5 (continuous and binned)
x_train <- model.matrix(avg_chip_seconds ~ scaled_ozone + scaled_pm25 + 
                        ozone_bin + pm25_bin, data = train_data)[,-1]  # remove intercept
y_train <- train_data$avg_chip_seconds

x_test <- model.matrix(avg_chip_seconds ~ scaled_ozone + scaled_pm25 + 
                       ozone_bin + pm25_bin, data = test_data)[,-1]
y_test <- test_data$avg_chip_seconds

# Run LASSO with cross-validation to select lambda
set.seed(123)
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, standardize = FALSE)  # already scaled

# Find the best lambda
best_lambda <- lasso_cv$lambda.min
best_lambda

# Fit final LASSO model at best lambda
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = best_lambda, standardize = FALSE)

# Check coefficients
coef(lasso_model)

# Predict on test set
preds <- predict(lasso_model, newx = x_test)

# Evaluate performance
rmse <- sqrt(mean((y_test - preds)^2))
mae  <- mean(abs(y_test - preds))
rmse
mae

```


We can see that both binned features and scaled_pm25 were retained showing (.). We can also see that scaled_ozone shrank to 0, which means it did not contribute to the predicting average chip time once the binned values were also included. The model achieved an RMSE of 3383 seconds and a MAE of 2861 seconds, suggesting that good predictive performance. 

Removing ozone based on LASSO results (KB)
```{r}
# Remove scaled_ozone from training and test data
train_data <- train_data[, !(names(train_data) %in% c("scaled_ozone"))]
test_data  <- test_data[, !(names(test_data) %in% c("scaled_ozone"))]

str(train_data)
str(test_data)
```

We can see that scaled ozone was successfully removed from both train and test data. 






(I ENDED HERE... I made the pre-feat-data into a csv and added the scaling and adding bins above)


```{r}
#Initial Model (MH)
lm_initial_scaledPM25 <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure + scaled_co + scaled_pm25 + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = train_data)


summary(lm_initial_scaledPM25)
plot(lm_initial_scaledPM25)

lm_initial_PM25bin <- lm(avg_chip_seconds ~ marathon + gender + subgroup + supershoe + scaled_avg_temp + scaled_precipitation + scaled_dew_point + scaled_wind_speed + scaled_visibility + scaled_sea_level_pressure + scaled_co + pm25_bin + ozone_bin + scaled_no2 + scaled_temp_aqi_interaction + scaled_avg_temp_gender_interaction, data = train_data)


summary(lm_initial_PM25bin)
plot(lm_initial_PM25bin)

```

```{r}
#Adding RMSE and MAE for looking at overfitting with test results below (MH/ZD)
residuals_lm_initial_PM25bin <- lm_initial_PM25bin$residuals
residuals_lm_initial_PM25bin_rmse <- sqrt(mean(residuals_lm_initial_PM25bin^2))
residuals_lm_initial_PM25bin_mae <- mean(abs(residuals_lm_initial_PM25bin))

residuals_lm_initial_scaledPM25 <- lm_initial_scaledPM25$residuals
residuals_lm_initial_scaledPM25_rmse <- sqrt(mean(residuals_lm_initial_scaledPM25^2))
residuals_lm_initial_scaledPM25_mae <- mean(abs(residuals_lm_initial_scaledPM25))



model_compare_train <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE  = c(residuals_lm_initial_scaledPM25_rmse,
            residuals_lm_initial_PM25bin_rmse),
  MAE   = c(residuals_lm_initial_scaledPM25_mae,
            residuals_lm_initial_PM25bin_mae),
  R2    = c(summary(lm_initial_scaledPM25)$r.squared,
            summary(lm_initial_PM25bin)$r.squared)
)

# Display nicely formatted table
knitr::kable(model_compare_train, digits = 4,
             caption = "Training Data Comparison: Continuous vs Binned PM2.5")

```





```{r}
# More Modeling (ZD)

# build a function
evaluate <- function(model, test_data) {
  preds <- predict(model, newdata = test_data)
  actual <- test_data$avg_chip_seconds
  
  rmse <- sqrt(mean((preds - actual)^2))
  mae  <- mean(abs(preds - actual))
  r2   <- cor(preds, actual)^2
  
  return(list(RMSE = rmse, MAE = mae, R2 = r2))
}

# Evaluate Model 1 with scaled PM2.5
results_scaledPM25 <- evaluate(lm_initial_scaledPM25, test_data)

# Evaluate Model 2 with PM2.5 bin
results_PM25bin <- evaluate(lm_initial_PM25bin, test_data)
```

```{r}
# compare results (ZD)
model_compare <- data.frame(
  Model = c("Continuous PM2.5", "PM2.5 Bin"),
  RMSE = c(results_scaledPM25$RMSE, results_PM25bin$RMSE),
  MAE  = c(results_scaledPM25$MAE,  results_PM25bin$MAE),
  R2   = c(results_scaledPM25$R2,   results_PM25bin$R2)
)

knitr::kable(model_compare, digits = 4,
             caption = "Comparison of Initial Models: Continuous vs Binned PM2.5")
```

```{r}
# build residual plots (ZD)
plot(lm_initial_PM25bin, which = 1)   # Residuals vs Fitted
plot(lm_initial_PM25bin, which = 2)   # Qâ€“Q plot
```


Modeling stopped here, what else is needed?

Think we can remove beyond this point!





























```{r}
# dont think this is needed, can probably delete this
lm_initial <- lm(
  avg_chip_seconds ~ marathon + gender + subgroup +
    supershoe +
    avg_temp + precipitation + dew_point +
    wind_speed + visibility + sea_level_pressure +
    pm25_bin + ozone_bin +
    temp_aqi_interaction + avg_temp_gender_interaction,
  data = train_data
)

summary(lm_initial)
```

```{r}
# can probably delete this too
pred <- predict(lm_initial, newdata = test_data)

rmse <- sqrt(mean((pred - test_data$avg_chip_seconds)^2))
r2   <- cor(pred, test_data$avg_chip_seconds)^2

rmse
r2
```
```{r}
# use environmental predictors both continuous and binned predictors
env_predictors <- c(
  "avg_temp", "temp_bin",
  "ozone", "ozone_bin",
  "pm25", "pm25_bin",
  "wind_speed", "wind_bin"
)
```

```{r}
# build a multiple regression model
evaluate_model <- function(predictors, train_data, test_data) {
  
  formula_string <- paste(
    "avg_chip_seconds ~ marathon + gender + subgroup + supershoe +",
    paste(predictors, collapse = " + ")
  )
  
  form <- as.formula(formula_string)
  
  model <- lm(form, data = train_data)
  
  preds <- predict(model, newdata = test_data)
  actual <- test_data$avg_chip_seconds
  
  rmse <- sqrt(mean((preds - actual)^2))
  r2   <- cor(preds, actual)^2
  mae  <- mean(abs(preds - actual))
  
  return(data.frame(
    Predictors = paste(predictors, collapse = ", "),
    NumPredictors = length(predictors),
    RMSE = rmse,
    R2 = r2,
    MAE = mae
  ))
}
```


```{r}
# store results in a list
all_results <- list()
i <- 1

# build loop to try combinations of predictors for 1 through 4 combinations
for (k in 1:4) {
  
  combos <- combn(env_predictors, k, simplify = FALSE)
  
  for (combo in combos) {
    all_results[[i]] <- evaluate_model(combo, train_data, test_data)
    i <- i + 1
  }
}

model_subset_results <- bind_rows(all_results)
```

```{r}
# print the results
model_subset_results_sorted <- model_subset_results[
  order(model_subset_results$RMSE), ]

knitr::kable(
  model_subset_results_sorted,
  digits = 4,
  caption = "Model Comparison Across All Subsets of Predictors"
)
```
Initial Modeling Results Above






Think we can delete beyond this point, going to leave it all for now. Started commenting out the below:
```{r}
# updated environmental predictors
#env_vars <- c(
#  "avg_temp", "precipitation", "dew_point",
#  "wind_speed", "visibility", 
#  "sea_level_pressure", "pm25", "no2", "ozone"
#)

#env_data <- main_data %>%
#  select(all_of(env_vars))

#pca_env <- prcomp(env_data, center = TRUE, scale. = TRUE)
#summary(pca_env)
```

```{r}
#library(knitr)

#pca_var <- data.frame(
#  PC = paste0("PC", 1:length(pca_env$sdev)),
#  Eigenvalue = pca_env$sdev^2,
#  Proportion_Variance = (pca_env$sdev^2) / sum(pca_env$sdev^2),
#  Cumulative_Variance = cumsum((pca_env$sdev^2) / sum(pca_env$sdev^2))
#)

#kable(
#  pca_var,
#  digits = 3, # can adjust after
#  caption = "Table Y: Variance Explained by Principal Components for Environmental Variables"
#)
```

```{r}
#library(ggplot2)

#ggplot(pca_var, aes(x = as.numeric(gsub("PC", "", PC)), y = Proportion_Variance)) +
#  geom_point() +
#  geom_line() +
#  labs(
#    title = "Figure Y: Scree Plot for Environmental PCA",
#    x = "Principal Component",
#    y = "Proportion of Variance Explained"
#  ) +
#  theme_minimal()
```

```{r}
#pca_scores <- as.data.frame(pca_env$x[, 1:3])
#colnames(pca_scores) <- c("PC1_env", "PC2_env", "PC3_env")

#main_data_pca <- main_data %>%
#  bind_cols(pca_scores)
```

```{r}
#main_data_pca <- main_data_pca %>%
#  mutate(
#    marathon = factor(marathon),
#    gender = factor(gender),
#    subgroup = factor(subgroup),
#    supershoe = factor(supershoe)
#  )
```

```{r}
#set.seed(123)

#train_index <- sample(1:nrow(main_data_pca), size = 0.9 * nrow(main_data_pca))

#train_data <- main_data_pca[train_index, ]
#test_data  <- main_data_pca[-train_index, ]
```

```{r}
lm_full <- lm(
  avg_chip_seconds ~ marathon + gender + subgroup +
    supershoe +
    avg_temp + precipitation + dew_point +
    wind_speed + visibility + sea_level_pressure +
    pm25 + no2 + ozone +
    temp_aqi_interaction + avg_temp_gender_interaction,
  data = train_data
)

summary(lm_full)


#lm_full <- lm(
#  avg_chip_seconds ~ marathon + gender + subgroup2 +
#    supershoe_era + covid_era +
#    high_temp + low_temp + avg_temp +
#    precipitation + dew_point + wind_speed +
#    sea_level_pressure + pm25 + no2 + ozone +
#    temp_dew_interaction + temp_aqi_interaction + 
#    temp_precip_interaction + temp_wind_interaction + pm25_temp_interaction + dew_wind_interaction #+     pressure_temp_interaction + avg_temp_gender_interaction,
#  data = train_data
#)

#summary(lm_full)
```

```{r}
# Predictions
lm_full_pred <- predict(lm_full, newdata = test_data)

# versus Actual
y_test <- test_data$avg_chip_seconds

# RMSE
lm_full_rmse <- sqrt(mean((lm_full_pred - y_test)^2))

# R squared on test set (correlation)
lm_full_r2 <- cor(lm_full_pred, y_test)^2

lm_full_rmse
lm_full_r2
```

```{r}
lm_pca <- lm(
  avg_chip_seconds ~ marathon + gender + subgroup +
    supershoe + PC1_env + PC2_env + PC3_env,
  data = train_data
)

summary(lm_pca)
```

```{r}
lm_pca_pred <- predict(lm_pca, newdata = test_data)

lm_pca_rmse <- sqrt(mean((lm_pca_pred - y_test)^2))
lm_pca_r2   <- cor(lm_pca_pred, y_test)^2

lm_pca_rmse
lm_pca_r2
```

```{r}
model_compare <- data.frame(
  Model = c("Full Linear Model", "PCA Linear Model"),
  RMSE  = c(lm_full_rmse, lm_pca_rmse),
  R2    = c(lm_full_r2,   lm_pca_r2)
)

kable(model_compare, digits = 3,
      caption = "Table Z: Model Performance Comparison on Test Set")
```


```{r}
# create a random forest
library(randomForest)

set.seed(123)

rf_model <- randomForest(
  avg_chip_seconds ~ marathon + gender + subgroup +
    supershoe +
    avg_temp + precipitation + dew_point + 
    wind_speed + visibility + sea_level_pressure +
    pm25 + no2 + ozone +
    temp_aqi_interaction + avg_temp_gender_interaction,
  data = train_data,
  ntree = 500,
  mtry = 5,
  importance = TRUE
)

rf_model
```

```{r}
rf_pred <- predict(rf_model, newdata = test_data)

rf_rmse <- sqrt(mean((rf_pred - y_test)^2))
rf_r2   <- cor(rf_pred, y_test)^2

rf_rmse
rf_r2

```


